[
["index.html", "Level 1 Data Skills Overview", " Level 1 Data Skills psyTeachR 2019-07-31 Overview This resource outlines the structure and content of our level 1 practical course at the School of Psychology at the University of Glasgow. We have developed these with a few key principles in mind: Pre, in-class and assessment. Each practical class, which we call labs in the student’s timetable and in this book, has prep material for the students to read and work through to provide context and additional support before the students attend class and work on the the in-class activites. These can take the form of, for example, videos made by the course lead informally talking through the tasks while live coding, papers related to the data being used, lectures related to the topic of the lab. These prep and in-class activities are closely linked to the assessments to enable skills to develop through practice. We also hold drop in practice sessions twice a week where Graduate Teaching Assistants (GTAs) are present to guide students to prep and in-class activites that will help in assessment completion while taking a hands-off the keyboard approach. The assessment enables the students to demonstrate their competency of the skills learnt in class. However, we take the perspective of skills develop through practice therefore each assessment builds on the work not only completed in the associated lab but provides an opportunity to demomstrate skill acquisition of skills learnt to date over the previous labs. Students will have received the solution for the previously attempted lab as a minimum, grade and feedback as well as the solution if feedback and grade turnaround permits, so this is an excellent opportunity for them to demonstrate integration of feedback and related skill development. Live coding is an important part of our teaching. We structure our classes with a staff member leading with support from two Graduate Teaching Assistants (GTAs). This enables the staff member to lead the class through the content while the GTAs can ensure the individual students are being challenged and supported adequately. The structure of skill building over labs supports this mode of teaching with the focus being on what each component of the code enables us to do rather than memorising code. Additional lab activites, such as the building of a portfolio over each semester, aims to encourage our students to focus on the understanding of the aim of the task the obtaining the code rather than memorising code only. The code that you see in the following chapters is the code staff were advised to use when live coding in class. Students were asked to open up a script and then staff live coded whilst talking through each task with GTA support ensuring students were confidently on task. The overall goal for our students in our year 1 Psychology labs *To understand the current state of psychological science and what Open Science is as well as its importance *To be clear on the importance of being confident and competent in data management *To be confident and competent at using RStudio as a tool to acheive good data management skills We have not shared the assessments for the labs but they are closely related to the in-class activities so these can be used as assessment templates too with code remaining similar to in-class but data or variables of interest changing. Where we have had formative or optional tasks for students to complete these will be listed under post-lab activities. You may notice a progression between labs 1 to 4 and labs 5 to 8. This is due to labs 1 to 4 building on skills progressively with associated increase in assessment weighting whilst labs 5 to 8 have a more consistent yet higher weighting. "],
["lab-1.html", "Lab 1 Lab 1 1.1 Pre-class activity 1.2 In-class activities 1.3 Additional resources around open science and replicability", " Lab 1 Lab 1 In this lab we will be introduced to the skills we will develop over the next year and beyond. This lab will introduce open science and why it is relevant to your development as a Psychologist. We will meet our groups and the staff that will be leading our labs and who we will get to know well over these two semesters. We can’t wait to get started! 1.1 Pre-class activity Watch this video of Dr Simine Vazire talking about open science and why it is an important concept for us to understand in Psychology. Intro to open science 1.2 In-class activities We want to give students more context about the importance of taking an open and replicable position with their research so we provide a simple drawing task where each of them has to follow the instructions below. The output is varied as prevoius staff attempts show! This activity is included in lab 1 so students have not yet been introduced to R but rather are learning abdout the issues and current debates in the field. 1.2.1 Activity 1: Replication task In your groups please complete these steps on a bit of paper. *Draw a square *Draw another square at 15 degree angle to the bottom of the 1st square *Draw 4 lines down from each corner of the 2nd square *Draw a circle next to 2nd square *Draw a smaller circle on top of first circle *Draw upward line from side of 1st circle *Draw 2 triangles on top of 2nd circle Two examples of different interpretations of this task: 1.2.2 Activity 2: Getting to know the data This semester we will use the same dataset each week to develop our skills and knowledge. The dataset is from Woodworth et al. (2018) details of which can be found here Web-based Positive Psychology Interventions: A Reexamination of Effectiveness. Familiarise yourself with the data and the study it comes from. What are the variables? Can you think of a research question that you would be interested in answering with this data? Post class activity This is a formative task where the students have to familiarise themselves with the data and develop a research question relative to that data specifically whilst thinking through what they would need the data to look like before analysing. For example, they may look at the data and be interested in seeing there was an effect of intervention type on happiness score so, in their groups, they would start to think about how they could wrangle their data to make it suitable for an appropriate analysis. Our thinking behind this was that by understanding the problem students will understand the need for an effective tool to help them. Although RStudio has not been introduced to them yet, open science and replicability has. We are presenting the challenge of big, messy real data before we present the tool that can meet this challenge - R. 1.3 Additional resources around open science and replicability Is most published research wrong? Replicability and Reproducibility Debate with Professor Dorothy Bishop Why an Entire Field of Psychology Is in Trouble "],
["lab-2.html", "Lab 2 Lab 2 2.1 Pre-class activities 2.2 In-class activities", " Lab 2 Lab 2 This is the first lab that the students will be introduced to R. The important point here is that R is introduced as a tool which will support their open science practices such as replicability and follows on from discussion in lab 1 about working with data effectively. We present the large messy data set in lab 1 to highlight the nature of work that students need to start thinking about before introducing R as the solution in lab 2. 2.1 Pre-class activities 2.1.1 Activity 1: Data Management Watch Intro to RStudio video Remember these key points as you work through the data activities on this course RStudio is a tool that enables you to become confident and competent working with data This is essential as a psychologist as you need to know that the data that findings are based on are reliable and valid. Imagine delivering a treatment that was based on research where the analyst was not confident or competent? See this tweet from Dr Dale Barr on how applicable these skills will be for you in your future careers… Figure 2.1: A tweet from Dale Barr These are skills that you will use beyond our undergraduate course and that are desireable to employers. Just ask our previous students…. Figure 2.2: A tweet from Steph Allan Think of a skill you nailed the first time you did it. Not so easy? You will learn to work with data by repeating the essential basic skills over and over. It’s new and will take time. Give yourself credit for trying and don’t stress if it doesn’t work right away. Use the resources we give, that are being provided online among the open science community, and ask questions in class and on the forums. We are here to help! 2.1.2 Activity 2: Setting the working directory What this means is that we tell R where the files we need are located. Think of it just like when you have different subjects, and you have seperate folders for each topic e.g. biology, history and so on. When working on R, it’s useful to have all the data sets and files you need in one folder. To set the working directory press session -&gt; set working directory -&gt; choose directory and then select the folder where the data sets we are working on are saved, and save this file in the same folder as well. In other words- make sure your data sets and scripts are all in the same folder. Follow the pre-lab video to help you with this too. In the labs, we recommend that you create a folder for Psychology labs with sub-folders within for each lab in your M: drive. This is your personal area on the University network that is safe and secure so is much better than flashdrives or desktops. You can access your M drive by logging into any computer on the University network. Figure 2.3: Setting your working directory 2.1.3 Activity 3: Reading and writing code Code consists of 4 building blocks: Functions - Functions do something with an input (or “argument”), and return an output. An example is the function read_csv(), which takes a file ending with .csv, and returns the data in a form RStudio can understand. You can usually recognise functions by their use of brackets. Arguments - Arguments are the input to functions. An example input to the function read_csv() might be \"mydata.csv\". Functions can often take multiple arguments, and can be either named (e.g. read_csv(file=\"mydata.csv\")) or unnamed read_csv(\"mydata.csv\"). Variables - Variables store information. When we run a function, we often want to store the output. For example, when might want to store the output of the read_csv() function in a variable called something like experiment_data. Assignment - Assignment is how we send the output from a function to our variable. We can do this by drawing an arrow from the function to the variable we want to store the output in, with &lt;-. This means that to store some csv data in a variable in RStudio we would run something like: experiment_data &lt;- read_csv(&#39;experiment_data.csv&#39;) 2.1.3.1 Quickfire Questions Which of the following is most likely to be an argument? 35 read_csv() &lt;- An easy way to spot functions is to look for numbers brackets computers. The job of &lt;- is to send the output from the function to a/an argument variable assignment. 2.1.4 Important information on our homework worksheets So that we can see your progress in data management skills we are using a worksheet format for the homework only called R Markdown (abbreviated as Rmd) which is a great way to create dynamic documents with embedded chunks of code. These documents are self-contained and fully reproducible which makes it very easy to share. This is an important part of your open science training as one of the reasons we are using RStudio is that it enables us to share open and reproducible information. The reason we are using these worksheets in this format is so that you are given a task and then can fill in the required code. For more information about R Markdown feel free to have a look at their main webpage sometime http://rmarkdown.rstudio.com. The key advantage of R Markdown is that it allows you to write code into a document, along with regular text, and then ‘knit’ it using the package knitr to create your document as either a webpage (HTML), a PDF, or Word document (.docx). Before submitting your homework file check that it knits OK to html then submit the .Rmd file, not the knitted html version You won’t be creating webpages or other document types (unless you want to!) just yet but we want to tell you a little more about the format of these worksheets so you know how and why we are using them. It enables you to enter code after class for assessment. We can then use a program to mark these and return feedback to you. This is right in line with our ethos of open and reproducible science enabling us to show you a skill, let you try it, and then enable you to show us how good you are at it! There are just a couple of important rules we need you to follow to make sure this all runs smoothly. These worksheets need to you fill in your answers and not change any other information. For example, if we ask you to replace NULL with your answer, only write in the code you are giving as your answer and nothing else. To illustrate - Task 1 read in your data data &lt;- NULL The task above is to read in the data file we are using for this task - the correct answer is data &lt;- read_csv(data.csv). You would replace the NULL with: Solution to Task 1 data &lt;- read_csv(&quot;data.csv&quot;) This means that we can look for your code and if it is in the format we expect to see it in, we can give you the marks! If you decide to get all creative on us then we can’t give you the marks as ‘my_lab_Nov_2018.csv’ isn’t the filename we have given to you to use. So don’t change the file, variable or data frame names as we need these to be consistent. We will look for your answers within the boxes which start and end with ``` and have {r task name} in them e.g. ```{r tidyverse, messages=FALSE} library(tidyverse) ``` These are called code chunks and are the part of the worksheet that we can read and pick out your answers. If you change these in any way we can’t read your answer and therefore we can’t give you marks. You can see in the example above that the code chunk (the grey zone), starts and ends with these back ticks (usually found on top left corner of the keyboard). This code chunk has the ticks and text which makes it the part of the worksheet that will contain code. The {r tidyverse} part tells us which task it is (e.g., loading in tidyverse) and therefore what we should be looking for and what we can give marks for - loading in the package called tidyverse in the example above. If this changes then it won’t be read properly, so will impact on your grade. 2.1.5 Take home message The easiest way to use our worksheets is to think of them as fill-in-the-blanks and keep the file names and names used in the worksheet the same. If you are unsure about anything then use the forums on Moodle and Slack to ask any questions and come along to the practice sessions. We have also made a short video about R Markdown and why we use it for our lab work which you can find in the prep section of lab 2 on Moodle. There is an R Markdown cheatsheet which will support you playing around with R Markdown which you can find here https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf. 2.2 In-class activities Part of becoming a psychologist is asking questions and gathering data to enable you to answer these questions effectively. It is very important that we understand all aspects of the research process such as experimental design, ethics, data management and visualisation. In this class, you will be learning how to develop reproducible scripts. This means scripts that completely and transparently perform some analysis from start to finish in a way that yields the same result for different people using the same software on different computers. And transparency is a key value of science, as embodied in the “trust but verify” motto. When you do things reproducibly, others can understand and check your work. This benefits science, but there is a selfish reason, too: the most important person who will benefit from a reproducible script is your future self. When you return to an analysis after two weeks of vacation, you will thank your earlier self for doing things in a transparent, reproducible way, as you can easily pick up right where you left off. The use of reproducible scripts is relevant within the context of open science which is a big debate in the scientific community at the moment. Some classic psychological experiments have been found to not be replicable. See the links below for some discussion on this topic: Study delivers bleak verdict on validity of psychology experiment results Low replicability in psychological science As part of your skill development, it is very important that you work with data so that you can become confident and competent in your management and analysis of data. In the labs, we will work with real data that has been shared by other researchers. When we are working with data we will use software called RStudio (). Information about this software can be found here. R Studio is available on the computers in the lab but is free to download onto your own devices. Guidance on how to do this can be found RStudio download. Remember to watch the prep video on RStudio for a tour around the interface with Heather. 2.2.1 Getting our data ready to work with Today in the lab we will be working on loading libraries (opening the apps in the prep video) required to work with our data and then loading our data into RStudio before getting it organised into a sensible format that relates to our research question. OK, so what is our first step? Yup, thats right, we need to tell RStudio what packages we want to use and where to find all the files we need before we pull them in ready for work. 2.2.2 Activity 1: Loading in the package First, we have to load in the package, tidyverse which contains the functions we’re going to use (read_csv() and inner_join() are examples of tidyverse funcitons). We’ll go into more detail about what tidyverse is in Lab 3. Solution library(tidyverse) 2.2.3 Activity 2: Read in data Next, we want to load in the data we’ll be working with. We’re using the function read_csv(), and giving the file path to our .csv files. The &lt;- tells us that the output from this code is sent to the variables dat for our data, and pinfo, for our participant info. Solution dat &lt;- read_csv (&#39;files/ahi-cesd.csv&#39;) pinfo &lt;- read_csv(&#39;files/participant-info.csv&#39;) 2.2.4 Activity 3: Join the files together In the last step, we created two dataframes, dat and pinfo. Now, we want to join these together into dataframe. We can do this with the inner_join() function. This will add the data from columns from pinfo to the data in dat, so that each row will contain a single participant’s data for one observation, as well as information like that participant’s sex. We’re saving the output of this to the variable, all_dat. Solution all_dat &lt;- inner_join(dat, pinfo, by=&#39;id&#39;, &#39;intervention&#39;) 2.2.5 Activity 4: Pull out variables of interest The select() function allows us to grab specific variables (columns) from a dataframe. summarydata will be the same as all_dat, but containing only the variables we asked for. Solution summarydata &lt;- select(all_dat, ahiTotal, cesdTotal, sex, age, educ, income, occasion, elapsed.days) Well done! You have started on your journey to become a confident and competent member of the open scientific community! To show us how competent you are you should now complete the assessment for this lab which follows the same instructions as this in-class activity but asks you to work with different variables. Always us the lab prep materials as well as what you do in class to help you complete the class assessments! Success through practice! "],
["lab-3.html", "Lab 3 Lab 3 3.1 Pre-class activity 3.2 In-class activities", " Lab 3 Lab 3 In this lab we move on from reading data in, joining tibbles and selecting variables of interest to becoming familiar with the Wickham 6 and the functionality of tidyverse. Practice is an important part of PsyTeachR so in our level 1 program we ask students to repeat tasks across the semester in the pre-class, in-class and assessments. This principle is carried over into the student’s independent study practices. We encourage students to do prep independently but also to come along to the practice sessions where space is open for students to work on lab tasks and assessments independently or discuss in groups with GTAs present to guide and support if needed. This is an important part of our community building and creating a safe space to practice, make mistakes and develop skills. This lab also contains the first of a selection of short informal videos as support materials. For this lab, the course lead talks through the verbs demonstrating their functions with data in real time. As these videos were specific to our level 1 cohort we haven’t shared them here but should emphasise that they received very positive feedback from the students who liked being able to return to them as they practiced their tidyverse skills. 3.1 Pre-class activity Watch Heather’s video on intro to tidyverse Work through the tidyverse book which talks you through the purpose of tidyverse using examples on a dataset called babynames. This package is installed on the lab computers but you can also install.packages(\"babynames\") on your own personal computer if you want to prep for the lab at home. Tidyverse (https://www.tidyverse.org/) is a collection of R packages created by world-famous data scientist Hadley Wickham. Tidyverse contains six core packages: dplyr , tidyr , readr , purrr , ggplot2 , and tibble. If you’ve ever typed library(tidyverse) into R, you will have seen that it loads in all of these packages in one go. Within these six core packages, you should be able to find everything you need to analyse your data. In this lab, we are going to focus on the dplyr package, which contains six important functions: select() Include or exclude certain variables (columns) filter() Include or exclude certain observations (rows) mutate() Create new variables (columns) arrange() Change the order of observations (rows) group_by() Organize the observations into groups summarise() Derive aggregate variables for groups of observations These six functions are known as ’single table verbs’ because they only operate on one table at a time. Later in the course you will learn two-table verbs that you can use to merge tables together. Although the operations of these functions may seem very simplistic, it’s amazing what you can accomplish when you string them together: Hadley Wickham has claimed that 90% of data analysis can be reduced to the operations described by these six functions. Remember these key points as you work through the data activities on this course RStudio is a tool that enables you to become confident and competent working with data This is essential as a psychologist as you need to know that the data findings are based on are reliable and valid. Imagine delivering a treatment that was based on research where the analyst was not confident or competent? Think of a skill you nailed the first time you did it. Not so easy? You will learn to work with data by repeating these basic skills over and over. It’s new and will take time. Give yourself credit for trying and don’t stress if it doesn’t work right away. Use the resources we give, that are being provided online among the open science community, and ask questions in class and on the forums. 3.2 In-class activities OK, so what is our first step? Yup, thats right, we need to tell RStudio where to find all the files we need and pull them in ready for work. Let’s go back to lab 2 and use our previous script to remember how to load the package, read in the data, and join the two data files together: 3.2.1 Activity 1: Packages and data Open up tidyverse and read in the data library(tidyverse) dat &lt;- read_csv (&#39;files/ahi-cesd.csv&#39;) pinfo &lt;- read_csv(&#39;files/participant-info.csv&#39;) all_dat &lt;- inner_join(dat, pinfo, by=&#39;id&#39;, &#39;intervention&#39;) Now lets get busy using our tidyverse verbs… 3.2.2 Activity 2: Select Select the columns all_dat, ahiTotal, cesdTotal, sex, age, educ, income, occasion, elapsed.days from the data and create a variable called summarydata. summarydata &lt;- select(all_dat, ahiTotal, cesdTotal, sex, age, educ, income, occasion, elapsed.days) 3.2.3 Activity 3: Arrange Arrange the data in the variable created above (summarydata) by ahiTotal with lowest score first. ahi_asc &lt;- arrange(summarydata, by = ahiTotal) 3.2.4 Activity 4: Filter Filter the data ahi_desc by taking out those who are over 65 years of age. age_65max &lt;- filter(ahi_asc, age &lt; 65) 3.2.5 Activity 5: Summarise Then, use summarise to create a new variable data_median, which calculates the median ahiTotal score in this grouped data and assign it a table head called median_score. data_median &lt;- summarise(age_65max, median_score = median(ahiTotal)) 3.2.6 Activity 6: Group_by Group the data stored in the variable age_65max by sex, and store it in data_sex then use mutate to create a new column called Happiness which categorises participants based on whether they score above the median ahiTotal score for females. data_sex &lt;- group_by(age_65max, sex) happy_female &lt;- mutate(data_sex, Happiness_Female = (ahiTotal &gt; 74)) "],
["lab-4.html", "Lab 4 Lab 4 4.1 Pre-class activity 4.2 In-class activities", " Lab 4 Lab 4 This is the last lab of semester 1 where the students willl work with data. Lab 5, the last lab of the semester, is where they present their reflections on what they would tell their fresher selves facilitating reflection on how far their skills have come in just 10 weeks. In this lab, lab 4, the students will again load in data, use the tidyverse verbs and generate a plot. Our students are also supported by a short video made by the course lead about ggplot. 4.1 Pre-class activity In this lab, we’ll be recapping the tidyverse verbs we’ve learnt so far, and expanding integrating this knowledge with an introduction to plotting data. Why should we plot our data? Here is an interesting thread from our colleagues Dr Guillaume Rousselet and Prof Lisa DeBruine on the reasons why we should use plots and the benefits of using ggplot, the package we will use today in RStudio: https://twitter.com/robustgar/status/1025342313004974080 As Grolemund and Wickham tell us: Visualisation is a fundamentally human activity. A good visualisation will show you things that you did not expect, or raise new questions about the data. A good visualisation might also hint that you’re asking the wrong question, or you need to collect different data. Visualisations can surprise you, but don’t scale particularly well because they require a human to interpret them. (http://r4ds.had.co.nz/introduction.html) 4.1.1 dplyr Recap In Lab 3 we were introduced to the tidyverse package, dplyr, and its six important functions. As a recap, which function(s) would you use to approach each of the following problems? We have a dataset of 400 adults, but we want to remove anyone with an age of 50 years or more. To do this, we could use the group_by() mutate() select() arrange() filter() summarise() function. We are interested in overall summary statistics for our data, such as the overall average and total number of observations. To do this, we could use the mutate() group_by() arrange() summarise() filter() select() function. Our dataset has a column with the number of cats a person has, and a column with the number of dogs. We want to calculate a new column which contains the total number of pets each participant has. To do this, we could use the summarise() mutate() select() arrange() filter() group_by() function. We want to calculate the average for each participant in our dataset. To do this we could use the group_by() and arrange() arrange() and mutate() group_by() and summarise() filter() and select() functions. We want to order a dataframe of participants by the number of cats that they own, but want our new dataframe to only contain some of our columns. To do this we could use the arrange() and select() group_by() and mutate() select() and summarise() mutate() and filter() functions. 4.1.2 Intro to Plotting Being able to visualise our variables, and relationships between our variables, is a very useful skill. Before we do any statistical analyses or present any summary statistics, visualising our variables is: A quick and easy way to check our data make sense, and to identify any unusual trends. A way to honestly present the features of our data to anyone who reads our research. Plotting in R is especially easy and flexible, thanks to the tidyverse package, ggplot2. This lab will only cover the basics, but here is an example usage of ggplot2 to plot some data: ggplot(my_data, aes(x = height, y = weight)) + geom_point() Figure 2.2: Example plot of height by weight data ggplot() creates the ggplot object, which puts our data in a form that ggplot2 can understand. my_data is the dataset. This contains some (pretend) heights and weights of my participants. Each row is a different participant. The aes() function tells the plot which variables are plotted on which axes. On the x axis, we’ve put participants’ heights, and on the y axis, their weights. geom_point() is a geom object which says what to actually do with all this information. Here, we tell ggplot2 to plot a point for each row (participant) in the dataset. 4.2 In-class activities 4.2.1 Getting our data ready to work with Today in the lab we will be working with our data to generate a plot of two variables from the Woodworth et al. dataset. Before we get to generate our plot, we still need to work through the steps to get our data in the shape we need it to be in for our particular question. You have done these steps before so go back to the relevant script and use that to guide you through. Can you remember when we did this? OK, so what is our first step? Yup, thats right, we need to tell RStudio where to find all the files we need and pull them in ready for work. Let’s go back to labs 2 and 3 to use our previous script to fill in the code in a new script: 4.2.2 Activity 1: Packages and data Open up tidyverse and read in data Solution library(tidyverse) dat &lt;- read_csv (&#39;files/ahi-cesd.csv&#39;) pinfo &lt;- read_csv(&#39;files/participant-info.csv&#39;) all_dat &lt;- inner_join(dat, pinfo, by=&#39;id&#39;, &#39;intervention&#39;) summarydata &lt;- select(all_dat, ahiTotal, cesdTotal, sex, age, educ, income, occasion, elapsed.days) 4.2.3 Activity 2: Select Select the columns all_dat, ahiTotal, cesdTotal, sex, age, educ, income, occasion, elapsed.days from the data and creat variable summarydata. Solution summarydata &lt;- select(all_dat, ahiTotal, cesdTotal, sex, age, educ, income, occasion, elapsed.days) 4.2.4 Activity 3: Arrange Arrange the data in the variable created above (summarydata) by ahiTotal with lowest score first. Solution ahi_asc &lt;- arrange(summarydata, by = ahiTotal) 4.2.5 Activity 4:Filter Filter the data ahi_asc by taking out those who are over 65 years of age. Solution age_65max &lt;- filter(ahi_asc, age &lt; 65) 4.2.6 Activity 5: Group and summarise Group the data stored in the variable age_65max by sex, and store it in data_sex. Then, use summarise to create a new variable data_median, which calculates the median ahiTotal score in this grouped data and assign it a table head called median_score. (Hint: if you’re stuck, see this dplyr documentation). Solution data_sex &lt;- group_by(age_65max, sex) data_median &lt;- summarise(data_sex, median_score = median(ahiTotal)) 4.2.7 Activity 6: Mutate Use mutate() to create a new column called Happiness which categorises participants based on whether they score above or below the median ahiTotal score. Solution happy &lt;- mutate(data_sex, Happiness = (ahiTotal &gt; 73)) 4.2.8 The importance of plotting We are now at the point where we can create a plot showing us some data. Use ggplot() to create a scatter plot of . Solution ggplot(age_65max, aes(x = ahiTotal , y = cesdTotal)) + geom_point(colour=&quot;purple&quot;) Figure 4.1: Happiness scores for participants aged less than 65 years Great job! You have now worked with the essential basics of good practice in data wrangling! To show us how you have mastered this complete the assessment and submit by 12pm 1 week from your lab. "],
["lab-5.html", "Lab 5 Lab 5 5.1 Pre-class activity 5.2 Wrangling Data 5.3 The Autism Quotient (AQ) 5.4 Thinking through the problem 5.5 In class activity 5.6 Making the computer do the dirty work", " Lab 5 Lab 5 5.1 Pre-class activity 5.2 Wrangling Data It would be nice to always get data formatted in the way that you want it, but alas, one of the challenges as a scientist is dealing with Other People’s Data. People often structure data in ways that is convenient for data entry, but not very convenient for data analysis, and so much effort must be expended ’wrangling’ data into shape before you can do more interesting things with it. Also, performing analyses often requires pulling together data obtained from different sources: you have done this in semester 1 by combining the participant information with the depression and happiness data. In this lesson, we are going to give you some tips on how to structure data, and introduce strategies for transforming and combining data from different sources. 5.3 The Autism Quotient (AQ) To illustrate these concepts, we will use some actual survey data that was collected online using SurveyMonkey.com (Note: the data have been slightly cleaned up to make things a bit simpler; in reality, the data files were even messier than it appears!) For this research project, prospective participants completed the short 10-item version of the Autism-Spectrum Quotient (AQ) (Baron-Cohen, Wheelwright, Skinner, Martin, &amp; Clubley, 2001), which is designed to measure autistic traits in adults. The data from Survey Monkey was downloaded as a .csv file, a common text format that you should be familiar with by now. Table 1: The ten items on the AQ-10. Q No. Question Q1 I often notice small sounds when others do not. Q2 I usually concentrate more on the whole picture, rather than small details. Q3 I find it easy to do more than one thing at once. Q4 If there is an interruption, I can switch back to what I was doing very quickly. Q5 I find it easy to read between the lines when someone is talking to me. Q6 I know how to tell if someone listening to me is getting bored. Q7 When I’m reading a story, I find it difficult to work out the characters’ intentions. Q8 I like to collect information about categories of things. Q9 I find it easy to work out what someone is thinking or feeling just by looking at their face. Q10 I find it difficult to work out people’s intentions. Responses to each item were measured on a four-point scale: Definitely Disagree, Slightly Disagree, Slightly Agree, Definitely Agree. To avoid response bias, each question is scored according to one of two different question formats. For questions 1, 7, 8, and 10, a point is assigned for agreement (either “Slightly Agree” or “Definitely Agree”) and zero otherwise. We will call this format “F” (for forward). The remaining questions are reverse coded (format “R”): a point is assigned for disagreement (“Slightly Disagree” or “Definitely Disagree”) and zero otherwise. The AQ for each participant is just the total points across all 10 questions. The higher the AQ score, the more ’autistic traits’ they are assumed to exhibit. Our goal is to calculate an AQ score for each participant in the dataset. Download the data here: AQ Data The data should look like this: To preserve anonymity, each participant was given a unique number, coded by the variable Id, shown in the first column of the table. The rest of the columns contain the responses associated with that participant for each of the 10 question (Q1, Q2, Q3, …, Q10). This data format is known as wide format: each unit of analysis (participant) forms a single row in the table, and each observation on that unit forms a separate column. Wide format is often convenient for data entry, but for reasons that will soon become apparent, we will find this format inconvenient for the task of scoring the responses. 5.4 Thinking through the problem How would you go about calculating AQ scores for each participant? Of course, you could do this by hand, but you have 10 responses for each of 66 participants. That makes 660 responses to code by hand. Not only is this a horribly mind-numbing task, it is also one in which you will be prone to make errors, especially as you get bored and your mind begins to wander. Even if you are 99% accurate, that means you will still get about 7 of the scores wrong. Worst of all, this approach does not scale beyond small datasets. As this survey was done over the internet, it could have easily been the case that you ended up with 6,600 participants instead of just 66. Learn how to make the computer do the mind-numbing tasks; it won’t make mistakes, and will free up your mind to focus on the bigger issues in your research. These investments in building your data science skills will pay off handsomely in the long run as these are skills we as researchers use everyday. OK, let’s imagine we are doing the task by hand so that we understand the logic. Once that logic is clear, we’ll go through it again and show you how to write the script to make it happen. Let’s take stock of what we know. First, we know that there are two question formats, and that questions Q1, Q7, Q8, and Q10 are scored according to format F and questions Q2, Q3, Q4, Q5, Q6, and Q9 are scored according to format R. We can represent this information in the table below: Question QFormat Q1 F Q2 R Q3 R Q4 R Q5 R Q6 R Q7 F Q8 F Q9 R Q10 F We also know that for format F, we award a point for agree, zero for disagree; and for format R, a point for disagree, zero for agree. We can represent that information as: QFormat Response Score F Definitely Agree 1 F Slightly Agree 1 F Slightly Disagree 0 F Definitely Disagree 0 R Definitely Agree 0 R Slightly Agree 0 R Slightly Disagree 1 R Definitely Disagree 1 Now that we have put that information into tables, it just becomes a simple matter of looking up the format based on the question number (Q1, Q2, …, Q10); given the format and the response, we can then assign the score. Let’s walk through the example with the first participant. For this participant (Id = 16), we have the following responses: Question Response Q1 Slightly Disagree Q2 Definitely Agree Q3 Slightly Disagree Q4 Definitely Disagree Q5 Slightly Agree Q6 Slightly Agree Q7 Slightly Agree Q8 Definitely Disagree Q9 Slightly Agree Q10 Slightly Agree Note that we have re-formatted the responses so that each response is in a separate row, rather than having all of the responses in a single row, as above. We have reshaped the data from its original wide format to long format. Obviously, this format is called ’long’ because instead of having just one row for each participant, we will now have 10 rows for a total of 66 x 10 = 660. While this format makes it less easy to take the whole dataset in with a single glance, it actually ends up being much easier to deal with, because ’Question’ is a now a single variable whose levels are Q1, Q2, …, Q10, and ’Response’ is also now a single variable. Most functions that you will be working with in R will expect your data to be in long rather than wide format. Let’s now look up the format for each question: Question Response QFormat Q1 Slightly Disagree F Q2 Definitely Agree R Q3 Slightly Disagree R Q4 Definitely Disagree R Q5 Slightly Agree R Q6 Slightly Agree R Q7 Slightly Agree F Q8 Definitely Disagree F Q9 Slightly Agree R Q10 Slightly Agree F And now that we have the format and the response, we can look up the scores: Question Response QFormat Score Q1 Slightly Disagree F 0 Q2 Definitely Agree R 0 Q3 Slightly Disagree R 1 Q4 Definitely Disagree R 1 Q5 Slightly Agree R 0 Q6 Slightly Agree R 0 Q7 Slightly Agree F 1 Q8 Definitely Disagree F 0 Q9 Slightly Agree R 0 Q10 Slightly Agree F 1 Then we just add up the scores, which yields an AQ score of 4 for participant 16. We would then repeat this logic for the remaining 65 participants.Anyone fancy doing this for a big data set?! We will work on using R to get our data into shape in lab 1 and introduce pipes in lab 2 which are a more efficient way of joining functions together. 5.5 In class activity 5.6 Making the computer do the dirty work Now let’s continue what we started in the prep by hand but now using R to change the data from wide to long format. First, we will need to create a new working directory and download the data files. responses.csv : survey responses qformats.csv : table linking questions to question formats scoring.csv : table linking formats and responses to score pinfo.csv : participant information (Sex, Age, etc.) Make a new R script, and make sure to set your working directory where those files can be located and load in tidyverse at the top of your R script then load in the three first files. responses &lt;- read_csv(&quot;files/responses.csv&quot;) # survey responses qformats &lt;- read_csv(&quot;files/qformats.csv&quot;) # question formats scoring &lt;- read_csv(&quot;files/scoring.csv&quot;) # scoring info Whenever you load in data, it is always a good idea to have a quick look to make sure things make sense (using glimpse(), View() or just clicking on them in the environment). For example, we can view the responses table by just typing the name responses in the console. Do that now. Now we can use R to transform our data from wide to long. We will use the function gather() function but we will do it for one participant just now. Some info on this gather() function can be found here What does gather() do?. rlong &lt;- gather(responses, Question, Response, Q1:Q10) What this means is that we have created a tibble called rlong using the function gather. We want gather to look in a previously craeted tibble called responses and then pull out the variables ‘Question’ and ‘Response’ for questions 1 to 10. Q1:Q10 is just a convenient shorthand for Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q10. We have now created a tibble with 660 observations over 3 variables; 10 observations per 66 participants on 3 variables. Let’s make it simpler just now and only use one participant. We can do that by using filter() which you used last semester and creating a new tibble called rlong_16. rlong_16 &lt;- filter(rlong, Id == 16) We should now have a tibble with 10 observations of 3 variables. OK, the next step: match each question with a format F or R. We have this information in the table qformats. So how do we combine these two tables? We want to match up the tables on the column Question. We can do this using an inner_join(). Try it and see what happens: rlong2 &lt;- inner_join(rlong_16, qformats, &quot;Question&quot;) Magic! The inner_join(), in effect added a new column to rlong which had the format of each question. Let’s look at the function call again…. The first two arguments to inner_join(), rlong and qformats, are the names of the tibbles that we want to join together. The next argument tells R how to join them, by Question. What the inner join does is match up rows in the two tables where both tables have the same value for the field named in the third argument, “Question”; it then combines the columns from the two tables, copying rows where necessary. To state it more simply, what it does, in effect, is the following: For each row in rlong, it checks the value of the column Question, and looks for rows with the same value in qformats, and then essentially combines all of the other columns in the two tables for these matching rows. If there are unmatching values, the rows get dropped. The inner_join() is one of the most useful and time-saving operations in data wrangling so keep ptracticing as it will keep reappearing time after time. Now that we have matched up each question with its corresponding format, we can now “look up” the corresponding scores in the scoring table based on the format and the response. So what we need to do is to combine information in our new table, rlong2, and the scoring table. We do this, once again, with an inner join, and we will store the result in a new variable, rscores. rscores &lt;- inner_join(rlong2, scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) So here we have created a new tibble called rscores using inner_join to join rlong2 and scoring using columns QFormat and Response and you are at a point where you are ready to calculate the AQ scores for participant 16. You have used the function to calculate the total last semester. Can you think where? Look back in your scripts and .Rmd files to remember where you have used this and how you can use it to calculate the total for participant 16. "],
["lab-6.html", "Lab 6 Lab 6 6.1 Pre-class activity 6.2 Inclass activities", " Lab 6 Lab 6 This lab builds on the skills developed to date by using pipes to join lines of previously used tidyverse verbs. In addition to the pre-class activity, you might find it useful to revisit previous in-class activities, assessments and feedback to recap skills developed to date. 6.1 Pre-class activity 6.1.1 What are pipes? Pipes in R look like this: %&gt;% Pipes are a handy feature of the tidyverse that allow you to send the output from one function straight into another function. Specifically, they send the result of the function before %&gt;% to be the first argument of the function after %&gt;%. Here’s an imaginary example showing how using pipes can simplify our code: Without Pipes With Pipes library(tidyverse) raw_data &lt;- read_csv(&quot;my_data.csv&quot;) sel_data &lt;- select(raw_data, id, age, score) clean_data &lt;- filter(sel_data, age &gt;= 18) final_data &lt;- arrange(clean_data, score) library(tidyverse) final_data &lt;- read_csv(&quot;my_data.csv&quot;) %&gt;% select(id, age, score) %&gt;% filter(age &gt;= 18) %&gt;% arrange(score) As you can see, pipes save us a lot of typing, and can make our code much more readable. Thanks to pipes, we can also keep our environment clean of intermediary variables that we only ever use once (i.e., we have no need for the variables raw_data, sel_data, or clean_data; we just save the result straight to final_data). Pipes might seem a little confusing at first - one way to make it easier to understand what your code is doing is to translate what your code is doing into English. You could pronounce %&gt;% as then. For example, you might read the above code on the right (with pipes) as saying: “read the csv then select the columns id, age, and score then filter to only include ages of 18 or greater then arrange (sort) by score” As great as pipes are, there are two main cases when it probably makes sense to write your code without pipes, and save the results from each step to separate variables: When the output from a step partway through your pipeline is somehow useful. For example, you might want to check that a step worked as expected, or the information might be useful again later on. When you’re completing assessments and we want you to save the output from a certain step to a specific variable name (we want to check that the step worked properly to be able to give you the mark!). 6.1.1.1 Quickfire Questions Which of the following is a pipe? %\"]'> %&gt;% &amp; &gt; == Where do pipes send the result of the function that precedes the pipe? To the third argument of the 18th function. To Ibiza. To the last argument of the next function. To the first argument of the next function. Are pipes a super-cool feature of the tidyverse that makes your code more readable and less verbose? yes yes yes yes 6.2 Inclass activities 6.2.1 Activity 1: library We are going to be working with the Autism Quotient (AQ) data which you have already been introduced to in lab 5 and in the pre-class for this lab. The relevant data required to use this data comes in 3 separate files; the survey responses ‘responses.csv’, the question formats ‘qformats.csv’ and scoring info ‘scoring.csv’. To get us started, load in tidyverse. Solution # load in tidyverse by replacing this line library(tidyverse) 6.2.2 Activity 2 : read_csv Read in the data to create the objects responses, qformats and scoring using the corresponding data files for each tibble i.e. responses for the response data, qformats for the question formats and scoring for the scoring info. Solution responses &lt;- read_csv(&quot;files/responses.csv&quot;) qformats &lt;- read_csv(&quot;files/qformats.csv&quot;) scoring &lt;- read_csv(&quot;files/scoring.csv&quot;) Checkpoint Has the data now appeared in the environment? Take a look and make sure each objects looks as you would expect it to. Does the data look the same as it did in the .csv files? Is it in the format ideal for analysis (see lab 1 in-class and lab 2 prep for a clue)? 6.2.3 Activity 3: gather Let’s get this data into long format from wide format.Firstly, create rlong by using gather to show the question and response for each participant in long format (if you’re struggling to understand gather, see lab 5 for more explanation about what each argument is doing). Solution rlong &lt;- gather(data = responses, key = Question, value = Response, Q1:Q10) 6.2.4 Activity 4: inner_join (QFormat) Now use inner_join to add in the question format (QFormat) for each question and store it in rlong2. This should now have 4 columns showing Id, Question, Response and QFormat. Solution rlong2 &lt;- inner_join(rlong, qformats, &quot;Question&quot;) What should it look like? ## # A tibble: 6 x 4 ## Id Question Response QFormat ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 16 Q1 Slightly Disagree F ## 2 17 Q1 Definitely Agree F ## 3 18 Q1 Definitely Agree F ## 4 19 Q1 Definitely Agree F ## 5 20 Q1 Definitely Disagree F ## 6 21 Q1 Slightly Disagree F 6.2.5 Activity 5: inner_joint (scores) We can now get the score for each participant for each question. Use inner_join to add in the score for each question and store it in rscores. Solution rscores &lt;- inner_join(rlong2, scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) What should it look like? ## # A tibble: 6 x 5 ## Id Question Response QFormat Score ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 16 Q1 Slightly Disagree F 0 ## 2 17 Q1 Definitely Agree F 1 ## 3 18 Q1 Definitely Agree F 1 ## 4 19 Q1 Definitely Agree F 1 ## 5 20 Q1 Definitely Disagree F 0 ## 6 21 Q1 Slightly Disagree F 0 6.2.6 Activity 6 - group_by To calculate the AQ score for each participant, firstly we need to group the scores by Id. Using the tibble rscores defined in task 5, group the AQ scores by Id and store it in aq_scores1. Solution aq_scores1 &lt;- group_by(rscores, Id) 6.2.7 Activity 7: summarise Now we have the scores grouped by Id, we can use summarise to calculate the sum of each question’s score for each participant. Do this for aq_scores1 and store it in aq_scores2. Solution aq_scores2 &lt;- summarise(aq_scores1, AQ = sum(Score)) What should it look like? ## # A tibble: 6 x 2 ## Id AQ ## &lt;dbl&gt; &lt;dbl&gt; ## 1 16 4 ## 2 17 2 ## 3 18 3 ## 4 19 1 ## 5 20 2 ## 6 21 2 6.2.8 Actvity 8: pipes You might notice that we only actually use the tibble aq_scores1 once, and it’s not that useful to keep in our environment. Inspired by our new-found passion for pipes, we can actually complete activities 6 and 7 in one step! Starting with the tibble rscores (calculated in activity 5), use pipes to calculate the AQ scores for each participant. Name this tibble aq_scores (make sure not to overwrite aq_scores1 or aq_scores2). Solution aq_scores &lt;- rscores %&gt;% group_by(Id) %&gt;% summarise(AQ = sum(Score)) Checkpoint Does aq_scores look the same as aq-scores2? The code for activity 8 should do the same operations as activities 6 and 7, but in one step. As a result, aq_scores from activity 8 should be identical to aq_scores2 from activity 7. 6.2.9 Activity 9: ggplot Finally, show the distribution of the overall AQ scores by producing a histogram from aq_scores. Solution ggplot(aq_scores, aes(AQ)) + geom_histogram() What should it look like? Figure 6.1: Autism Quotient Score distribution. Well done! You should use this activity to help you complete the assessment to be submitted 1 week from your lab by 12pm. "],
["lab-7.html", "Lab 7 Lab 7 7.1 Pre-class activities 7.2 In-class activities", " Lab 7 Lab 7 Lab 7 and 8 introduce the students to probability. In lab 7, we introduce binomial probability and then in lab 8 we introduce normal distribution probability. Again, we use an informal video by the course lead to suppport students in understanding the theoretical concepts behind probability. 7.1 Pre-class activities 7.1.1 Activity 1: Video Watch the video on probability made by Heather where she talks through why learning about probability is relevant and how to calculate probabilities of events. 7.1.2 Probability and Probability Distributions The aim of Lab 7 is to introduce concepts and R/RStudio functions that you can use to: calculate probabilities create probability distributions make estimations from probability distributions. 7.1.3 Activity 2: library There are no cheatsheets for this lab as we will not be using a specific package. However, you can make full use of the R help function (e.g. ?sample) when you are not clear on what a function does. There are loads of videos and help pages out there with clear examples to explain difficult concepts. For now though, and to start this prep, open a new script and load in the tidyverse package (we need this for some visualisation work later). library(&quot;tidyverse&quot;) 7.1.4 Types of data How you tackle probability depends on the type of data/variables you are working with (i.e. discrete or continuous). This is also referred to as Level of Measurements. Discrete data can only take integer values (whole numbers). For example, the number of participants in an experiment would be discrete - we can’t have half a participant! Discrete variables can also be further broken down into nominal and ordinal variables. Ordinal data is a set of ordered categories; you know which is the top/best and which is the worst/lowest, but not the difference between categories. For example, you could ask participants to rate the attractiveness of different faces based on a 5-item Likert scale (very unattractive, unattractive, neutral, attractive, very attractive). You know that very attractive is better than attractive but we can’t say for certain that the difference between neutral and attractive is the same size as the distance between very unattractive and unattractive. Nominal data is also based on a set of categories but the ordering doesn’t matter (e.g. left or right handed). Nominal is sometimes simply refered to as categorical data. Continuous data on the other hand can take any value. For example, we can measure age on a continuous scale (e.g. we can have an age of 26.55 years), other examples include reaction time or the distance you travel to university every day. Continuous data can be broken down into interval or ratio data; but more of that another time. 7.1.5 Simple probability calculations Today, we are going to cover the concepts of probability calculations. Probability is the extent to which an event is likely to occur and is represented by a real number p between 0 and 1. For example, the probability of flipping a coin and it landing on ‘tails’ most people would say is estimated at p = .5. Calculating the probability of any discrete event occuring can be formulated as: \\[p = \\frac{number \\ of \\ ways \\ the \\ event \\ could \\ arise}{number \\ of \\ possible \\ outcomes}\\] For example, what is the probability of randomly drawing your name out of a hat of 12 names where one name is definitely yours? 1/12 ## [1] 0.08333333 The probability is .08, or to put it another way, there is an 8.3% chance that you would pull your name out of the hat. 7.1.6 Joint probability Describing the probability of single events, such as a coin flip or rolling a six is easy, but more often than not we are interested in the probability of a collection of events, such as the number of heads out of 10 flips or the number of sixes in 100 rolls. For simplicity, let’s take the example of a coin flip. Let’s say we flip a coin four times. Assuming the coin is fair (probability of heads = .5), we can ask question such as: what is the probability that we get heads on exactly one out of the four coin flips? 7.1.7 Activity 3: Estimate the probability of getting X heads over four independent flips. Let’s start by introducing the sample() function in R, which samples elements from a vector, “x”. A vector is a sequence of data elements of the same basic type. Members in a vector are officially called components. Let’s define a vector using c(\"HEADS\", \"TAILS\") and take four samples from it. # Notice that because our event labels are strings (text), # we need to enter them into the function as a vector; i.e. in &quot;quotes&quot; sample(x = c(&quot;HEADS&quot;, &quot;TAILS&quot;), size = 4, replace = TRUE) ## [1] &quot;TAILS&quot; &quot;TAILS&quot; &quot;HEADS&quot; &quot;TAILS&quot; What you have done here is tell R that you want to sample observations from a set of data, which in this case was our vector c(\"HEADS\", \"TAILS\"). size tells R the number of samples you want to take. We chose four, which is larger than the size of the vector itself (it has only two components; HEADS and TAILS). For this reason we need to override the default setting, which is sampling without replacement. Sampling without replacement means that once an observation appears in the sample, it is removed from the source vector so that it can’t be drawn again. Sampling with replacement means that an observation is retained so it can be drawn again. In this case, if we don’t override the default it would mean that if we tossed HEADS we wouldn’t be able to toss the coin and get HEADS again and we would get an error message because we’ve asked for more observations than are possible (we override this by setting replace to TRUE) It will make our lives easier if we define heads as one and tails as zero and write the same command as follows: # Now that our event labels are numeric, we don&#39;t need the combine function `c` # 0:1 means all numbers from 0 to 1 in steps of 1. So basically, 0 and 1. sample(0:1, 4, TRUE) ## [1] 0 1 0 1 7.1.8 Activity 4: sum This is logically equivalent, but using ones and zeroes means that we can count the number of heads by simply summing up the values in the vector. For instance: # The above statement pipes the output of sample() #then the function sum() which counts up the number of heads. sample(0:1, 4, TRUE) %&gt;% sum() ## [1] 3 Run this function multiple times (you can use the up arrow in the console to avoid having to type it over and over). I ran it five times the results of my little (N = 5) simulation of flipping four coins and counting heads were: 1, 3, 2, 2, and 3. So in only one out of the five simulations did I get exactly one heads. 7.1.9 Activity 5: replicate 1 Let’s repeat the experiment a whole bunch more times. We can have R do this over and over again using the replicate() function from base R. The basic syntax of replicate() is replicate(number_of_times, expression_you_want_to_replicate). So # repeats the expression sample(0:1, 4, TRUE) %&gt;% sum() 20 times; the result comes out as a vector with each number represents a count of the number of heads over four flips in each of the 20 ’experiments’. replicate(n = 20, expr = sample(0:1, 4, TRUE) %&gt;% sum()) ## [1] 3 0 3 2 1 1 1 1 3 1 2 1 2 1 1 3 1 2 0 2 7.1.10 Monte Carlo simulation Every year, the city of Monte Carlo is the site of innumerable games of chance played in its casinos by people from all over the world. This notoriety is reflected in the use of the term “Monte Carlo simulation” among statisticians to refer to use of computer simulation to estimate statistical properties of a random process. In a Monte Carlo simulation, the random process is repeated over and over again in order to assess its performance over the long run. It is usually used in situations where mathematical solutions are unknown or hard to compute. Now we are ready to use Monte Carlo simulation to attack the question of the probability of various outcomes. 7.1.11 Activity 6: replicate 2 We are going to run our coin flip experiment but this time we are going to run the experiment 50 times (each including 4 coin tosses), or collect 50 data points, and use the same principles to predict the number of heads we will get. Store the result in a variable heads50 using the code below: heads50 &lt;- replicate(50, sample(0:1, 4, TRUE) %&gt;% sum()) heads50 # this time we save the output in the variable heads50 # so we can work with the results further ## [1] 2 1 2 1 1 2 3 1 1 0 2 1 2 2 2 0 2 3 3 3 3 0 3 4 2 3 2 4 3 2 3 2 2 1 1 ## [36] 3 2 2 0 2 1 2 2 1 1 2 2 1 2 4 Each of these numbers represents the number of heads from 4 coin tosses, repeated 50 times. The first 2 data points tell us that they each produced heads twice. Your results may differ, since sample() is random. In this case, four of the experiments yielded the outcome of 4/4 heads. 7.1.12 Activity 7: probability Note that we can estimate the probability of each of the outcomes (0, 1, 2, 3, 4 heads after 4 coin tosses) by counting them up and dividing by the number of experiments. We will do this by putting the results of the replications in a tibble() and then using count(). data50 &lt;- tibble(heads = heads50) %&gt;% # convert to a tibble group_by(heads) %&gt;% # group by number of possibilities (0,1,2,3,4) summarise(n = n(), # count occurances of each possibility, p=n/50) # &amp; calculate probability (p) of each ## # A tibble: 5 x 3 ## heads n p ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 4 0.08 ## 2 1 12 0.24 ## 3 2 21 0.42 ## 4 3 10 0.2 ## 5 4 3 0.06 What we have done here is created a variable heads50 in a new tibble object that contains the results from our 50 experiments, grouped it by the values in heads50 (0, 1, 2, 3, or 4), and counted the number of times each value was observed in summarise(n = n()), and then calculated the probability by dividing the newly created variable n by the number of experiments (50). 7.1.13 Activity 8: visualisation We can then plot a histogram of the outcomes using ggplot2. Figure 7.1: No. of heads from 4 coin tosses probability outcomes. So the estimated probabilites are 0.12 for 0/4 heads, 0.32 for 1/4 heads, etc. Unfortunately sometimes this calculation will estimate that the probabilty of an outcome is zero since this outcome never came up when the simulation is run. If you want reliable estimates, you need a bigger sample as we know that this was a possible outcome but just didnt occur this time. 7.1.14 Activity 9: big data Let’s repeat the Monte Carlo simulation, but with 10,000 experiments instead of just 50. Here we are again using R to tackle big data and multiple calculations which is much faster and more efficient than if we had to figure this out by hand! All we have to do is change the 50 to 10000 in the above call to replicate. Again, we’ll put the vector in a tibble and calculate counts and probabilities of each outcome using group_by() and summarise(). heads10K &lt;- replicate(10000, sample(0:1, 4, TRUE) %&gt;% sum()) # this time we save the output in the variable heads10K data10K &lt;- tibble(heads = heads10K) %&gt;% group_by(heads) %&gt;% summarise(n = n(), p=n/10000) Remember to try reading your code in full sentences to help you understand what multiple lines of code connected by pipes are doing. How would you read the above code? ggplot(data10K, aes(heads,p)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;purple&quot;) + labs(x = &quot;Number of Heads&quot;, y = &quot;Probability of Heads in 4 flips (p)&quot;) Figure 7.2: 10K coin toss probability outcomes. Using Monte Carlo simulation, we estimate that the probability of getting exactly one head on four throws is about 0.25. The above result represents a probability distribution for all the possible outcomes in our experiments. We can derive lots of useful information from this. For instance, what is the probability of getting two or more heads in four throws? This is easy: the outcomes meeting the criterion are 2, 3, or 4 heads. We can just add these probabilities together like so: data10K %&gt;% filter(heads &gt;= 2) %&gt;% summarise(p2 = sum(p)) ## # A tibble: 1 x 1 ## p2 ## &lt;dbl&gt; ## 1 0.680 You can add probabilities for various outcomes together as long as the outcomes are mutually exclusive, that is, when one outcome occurs, the others can’t occur. For this coin flipping example, this is obviously the case: you can’t simultaneously get exactly two and exactly three heads as the outcome of a single experiment. However, be aware that you can’t simply add probabilities together when the events in question are not mutually exclusive: for example, the probability of the coin landing heads up, and it landing with the image of the head being in a particular orientation are not mutually exclusive, and can’t be simply added together. Additionally, the probabilities we get from Monte Carlo simulations are just estimates; we need something more definitive. 7.1.15 Theoretical probability distributions Recall that probability is not much more sophisticated than counting and dividing. We are now going to move this forward by looking at binomial distributions in lab 7 and normal distributions in lab 8. See you in lab 7! \\[p = \\frac{number \\ of \\ ways \\ the \\ event \\ could \\ arise}{number \\ of \\ possible \\ outcomes}\\] 7.2 In-class activities 7.2.1 Monte Carlo simulations Before we move on, make sure you are comfortable with Monte Carlo simulations that we learnt about in the prep materials. Remember, the staff and GTAs are in the lab to support your learning so if in doubt, just ask one of them to go through it with you. These are concepts that need some work and repetition, so take the time to get comfortable with them as that will put you in good stead moving forward with this lab and lab 4. 7.2.2 The Binomial Distribution - Creating a Discrete Distribution So far we have learnt how probabilities and distributions work using the simple probability calculation. However, if we wanted to calculate the probability of 8 heads from 10 coin flips we don’t have to go through this entire procedure each time. Instead, because we have a dichotomous outcome, heads or tails, we can calculate probabilities using the binomial distribution - effectively what you just created. We will walk through some essentials here. We’ll use 3 functions to work with the binomial distribution: dbinom() - the density function: gives you the probability of x successes given the number of trials and the probability of success on a single trial (e.g., what’s the probability of flipping 8/10 heads with a fair coin?). pbinom() - the distribution function: gives you the cumulative probability of getting a number of successes below a certain cut-off point (e.g. probability of getting 0 to 5 heads out of 10 flips), given the size and the probability. This is known as the cumulative probability distribution function or the cumulative density function. qbinom() - the quantile function: is the inverse of pbinom in that it gives you the x axis value below (and including the value) which the summation of probabilities is greater than or equal to a given probability p, plus given the size and prob. So let’s try these functions out to answer two questions: What is the probability of getting exactly 5 heads on 10 flips? What is the probability of getting at most 2 heads on 10 flips? 7.2.3 Activity 1: dbinom Let’s start with question 1, what is the probability of getting exactly 5 heads on 10 flips? We want to predict the probability of getting 5 heads in 10 trials (coin flips) and the probability of success is 0.5 (it’ll either be heads or tails so you have a 50/50 chance which we write as 0.5). We write that as: dbinom(x = 5, size = 10, prob = 0.5) ## [1] 0.2460938 The probability of getting 5 heads out of 10 coin flips is 0.25 (to 2 decimal places). which would look like this: Figure 7.3: 5 heads out of 10 coin toss probability outcomes. The dbinom (density binom) function takes the format of dbinom(x, size, prob), where the arguments we give are: *x the number of ‘heads’ we want to know the probability of. Either a single one, 3 or a series 0:10. In this case it’s 5. *size the number of trials (flips) we are doing; in this case, 10 flips. *prob the probability of ‘heads’ on one trial. Here chance is 50-50 which as a probability we state as 0.5 or .5 7.2.4 Activity 2: pbinom OK, question number 2….What is the probability of getting at most 2 heads on 10 flips? This time we use pbinom as we want to know the cumulative probability of getting a maximum of 2 heads from 10 coin flips so we have set a cut-off point of 2 but we still have a probability of getting a heads of 0.5. note that the argument name is q rather than x. This is because in dbinom x was a fixed number, whereas q is all the possibilities up to 2 (.e.,g 0, 1, 2). pbinom(q = 2, size = 10, prob = 0.5) ## [1] 0.0546875 The probability of us getting a maximum of 2 heads on 10 coin flips is 0.05 (2 decimal places). Note that there is another way we could have done this: probs &lt;- pbinom(q = 0:2, size = 10, prob = 0.5) probs ## [1] 0.0009765625 0.0107421875 0.0546875000 sum(probs) ## [1] 0.06640625 Note that in this latter strategy, q = 0:2 will give us the probabilties for 0, 1, and 2 heads separately and then we just add these up to calculate the cumulative probablity of this scenario. Figure 7.4: Max of 2 heads from 10 coin toss probability outcomes. Let’s try one more scenario with a cut-off point to make sure we have understood this. What is the probability of getting 7 or more heads on 10 flips? We can actually approach this in two different ways. We have done one way to answer the previous question which we can use again here but instead of using a maximum of 2 coin flips (0:2) we are setting the cut-off as 7 to 10 (7:10). ## [1] 3.933594 This doesn’t look quite right does it? Probabilities should be between 0 and 1 not 3.93. The default behaviour of pbinom is to tell us the probability of getting that number of flips or lower. That is, we have summed the probability of getting fewer than 7 heads, fewer than 8 heads, and so on, which is not what we wanted. We can also think about this in terms of the tails of the plots. Are we interested in the lower tail of the plot or the higher tail? The total area under the curve for a theoretical distribution sums to 1. The default behaviour for pbinom is to give us the cumulative probablity of the lower tail - if we want the upper tail instead, we set lower.tail to FALSE, and this will give us the probability for the blue bars. In this example, we will set lower.tail = FALSE as we are interested in the upper tail of the distribution. Because we want the cumulative probability to include 7, we set q = 6. ## [1] 0.171875 Figure 7.5: Minimum of 7 heads out of 10 coin toss probability outcomes. 7.2.5 Activity 3: qbinom OK, now let’s consider a scenario in which you’d use the quantile function qbinom. You suspect that the coin is biased against heads. Your null hypothesis is that the coin is not biased against heads (P(heads)=.5). You are going to run a single experiment to test your hypothesis, with 10 trials. What is the minimum number of successes that is acceptable if you want to keep your chances of getting heads for this type of experiment at .05? You have used the argument prob in the previous two functions, dbinom and pbinom, and it represents the probability of success on a single trial (here it is the probability of ‘heads’ in one coin flip, .5). For qbinom, prob still represents the probability of success in one trial, whereas p represents the overall probability of success across all trials. When you run pbinom, it calculates the number of heads that would give that probability. In other words, you ask it for the minimum number of successes (e.g. heads) to maintain an overall probability of .05, in 10 flips, when the probability of a success on any one flip is .5. ## [1] 2 And it tells you the answer is 2. qbinom also uses the lower.tail argument and it works in a similar fashion to pbinom. So if you got fewer than two heads, you would reject the null hypothesis that the coin was unbiased against heads. Ten trials is probably far too few. What would your cutoff be if you ran 100 trials? 1000? 10000? ## [1] 42 474 4918 Congratulations! You are well on you way to understanding binomial probability so complete the assessment for this lab and submit one week from attending by 12pm. Next time we will be looking at normal distributions and probability. "],
["lab-8.html", "Lab 8 Lab 8 8.1 Pre-class activity 8.2 Inclass activity", " Lab 8 Lab 8 8.1 Pre-class activity 8.1.1 Discrete or Continuous Datasets This is a short tutorial with just a couple of quick questions on how the level of measurement can alter the way in which you tackle probability. Levels of measurement can be categorised as either discrete or continuous: Discrete data can only take integer values (whole numbers). For example, the number of participants in an experiment would be discrete - we can’t have half a participant! Categorical variables, like handedness, can be similarly coded as integers (e.g. right handed = 0, left-handed = 1). Discrete variables can also be further broken down into nominal/categorical and ordinal variables. Nominal/categorical data is based on a set of categories but the ordering doesn’t matter (e.g. left or right handed). Ordinal data is based on a set of ordered categories; you know which is the top/best and which is the worst/lowest, but not the difference between categories. For example, you could ask participants to rate the attractiveness of different faces based on a 5-item Likert scale (very unattractive, unattractive, neutral, attractive, very attractive). Continuous data, on the other hand, can take any numeric value. For example, we can measure age on a continuous scale (e.g. we can have an age of 26.55 years). Other examples include reaction time or the distance you travel to university every day. Continuous variables can be further broken down into interval and ratio variables. Interval data is data which comes in the form of a numerical value where the difference between points is standardised and meaningful. For example temperature, the difference in temperature between 10-20 degrees is the same as the difference in temperature between 20-30 degrees. Ratio data is very like interval but has a true zero point. With our interval temperature example above, we have been experiencing negative temperatures (-1,-2 degrees) in Glasgow but with ratio data you don’t see negative values such as these i.e. you can’t be -10 cm tall. Here is a video with a useful summary on the differences between discrete and continuous data: https://youtu.be/6IdJ1aPFDCs. When you read journal articles or when you are working with data in the lab, it is really good practice to take a minute or two to figure out the type of variables you are reading about and/or working with. We have introduced these concepts in a little more depth as we are moving onto lab 8 and normal distribution probability so this will help with understanding this concept in relation to binomial probability and the different types of data involved. 8.1.1.1 Quickfire Questions What kinds of data are the following examples of? Time taken to run a marathon (in seconds): interval ordinal ratio categorical Finishing position in marathon (e.g. 1st, 2nd, 3rd): ordinal interval ratio categorical Which Sesame Street character a runner was dressed as: ratio categorical ordinal interval Temperature of a runner dressed in a cookie monster outfit (in degrees Celsius): ratio categorical interval ordinal 8.1.2 Binomial Distributions Recap The binomial distributions in the previous lab are used for calculating probabilities for situations where there are two possible outcomes. They can tell us, for example, the probability of observing any number of heads occurring if a coin is flipped a given number of times. A binomial distribution models the probability of any number of successes being observed, given the probability of a success and the number of observations. As the variable of “Number of Successes” is discrete (it is a count, so can only be a whole number), we plot the probabilities on a bar plot. 8.1.3 Introduction to the Normal Distribution The normal distribution, on the other hand, reflects the probability of any value occurring for a continuous variable. Examples of continuous variables include height or age, where a single person can score anywhere along a continuum. For example, a person could be 21.5 years old and 176cm tall. As the normal distribution models the probability of a continuous variable, we plot the probability using a density plot. A normal distribution looks like this: Figure 8.1: Normal Distribution of height. \\(\\mu\\) = the mean (average), \\(\\sigma\\) = standard deviation Normal distributions are symmetrical, meaning there is an equal probability of observations occurring above and below the mean. This means that, if the mean in figure 1 is 170, we could expect the number of people who have a height of 160 to equal the number of people who have a height of 180. This also means that the mean, median, and mode are all expected to be equal in a normal distribution. As with any probabilities, real-world data will come close to the normal distribution, but will (almost certainly) never match it exactly. As we collect more observations from normally-distributed data, our data will get increasingly closer to a normal distribution. As an example, here’s a simulation of an experiment in which we collect heights from 5000 participants. As you can see, as we add more observations, our data starts to look more and more like the normal distribution in the previous figure. Figure 2.1: A simulation of an experiment collecting height data from 2000 participants 8.1.3.1 Quickfire Questions Complete the sentences so that they are correct. In a normal distribution, the mean, median, and mode are always interval data are all equal sum to zero. An example of data in which you might expect to see a normal distribution is the number of heads observed out of 10 coin flips the number of goals scored by Leeds United F.C. the number of people who are right handed. Whereas the binomial distribution is based on situations in which there are two possible outcomes, the normal distribution is based on situations in which the data is a continuous variable has three possible values is a categorical variable. 8.1.4 Working with the Normal Distribution Just as we used functions to work with the binomial distribution (dbinom, pbinom, and qbinom), a similar set of functions exist to help us work with other distributions, including the normal distribution: dnorm()-the density function, for calculating the probability of a specific value pnorm()-the probability or distribution function, for calculating the probability of getting at least or at most a specific value qnorm()-the quantile function, for calculating the specific value associated with a given probability As you can probably see, these functions are very similar to the functions we’ve already come across, that are used to work with the binomial distribution. We’ll go through these concepts in more detail with the in-class activity, but you might want to try this web app, which introduces the functions in more detail: Normal Distributions Web App 8.2 Inclass activity In lab 7 and the pre lab for this lab you recapped and expanded on your understanding of probability, including a number of binom functions as well as some more basic ideas on probability and data types. You will need these skills to complete the following so please make sure you have carried out the pre class on types of data before attempting this. Remember to follow the instructions and if you get stuck at any point to ask the staff member and GTAs to talk you through it. Staff will talk you through the mean and standard deviations in relation to a normal distribution. 8.2.1 Working with the Normal Distribution We won’t ask you to create a normal distribution from scratch as it is more complicated than the binomial distribution you estimated in lab 7. Unlike coin flips, the outcome in the normal distribution is not just 50/50. Just as with the binomial distribution (and other distributions) there are functions that allow us to estimate the normal distribution and to ask questions about the distribution. These are: dnorm()-the density function, for calculating the probability of a specific value pnorm()-the probability or distribution function, for calculating the probability of getting at least or at most a specific value qnorm()-the quantile function, for calculating the specific value associated with a given probability They work in a similar way to their binomial counterparts. If you are unsure about how a function works you can call the help on it by typing in the console, for example, ?dnorm or ?dnorm(). In this lab we’ll focus on pnorm() and qnorm(). As mentioned in the pre-class activity, this interactive web app provides a good introduction to these functions: Normal Distributions Web App 8.2.2 Probability of Heights Data from the Scottish Health Survey (2008) shows that: The average height of a 16-24 year old Scotsman is 176.2 centimetres, with a standard deviation of 6.748. The average height of a 16-24 year old Scotswoman is 163.8 cm, with a standard deviation of 6.931. In this lab we will use this information to calculate the probability of observing at least or at most a specific height with pnorm(), and the heights that are associated with specific probabilities with qnorm(). 8.2.3 pnorm() - the probability or distribution function What is the probability of meeting a 16-24 y.o. Scotswoman who is taller than the average 16-24 y.o. Scotsman? Solution p_taller_w &lt;- pnorm(176.2, 163.8, 6.931, lower.tail = FALSE) Fiona is a very tall Scotswoman (181.12cm) in the 16-24 y.o. range who refuses to date men who are shorter than her. What proportion of all 16-24 Scotsmen would she be willing to date? Solution p_taller_m &lt;- pnorm(181.12, 176.2, 6.748, lower.tail = FALSE) Out of 100,000 16-24 y.o. Scotswomen, how many would you predict would meet the height eligibility requirement to join the Royal Navy (at least 151.5cm tall)? Solution n_sailors &lt;- 100000 * pnorm(151.5, 163.8, 6.931, lower.tail = FALSE) 8.2.4 qnorm() - the quartile function Let’s use our height example to work with qnorm: How tall would a 16-24 y.o. Scotsman have to be in order to be in the top 5% of the height distribution for Scotsmen in his age group? Solution height_cutoff &lt;- qnorm(.05, 176.2, 6.748, lower.tail = FALSE) "]
]
