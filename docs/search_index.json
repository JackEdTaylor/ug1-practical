[
["index.html", "ug1-practical Overview", " ug1-practical psyTeachR 2019-05-01 Overview This resource outlines the structure and content of our level 1 practical course at the School of Psychology at the University of Glasgow. We have developed these with a few key principles in mind: Pre, in-class and assessment. Each practical class, which we call labs in the student's timetable and in this book, has prep material for the students to read and work through to provide context and additional support before the students attend class and work on the the in-class activites. These can take the form of, for example, videos made by the course lead informally talking through the tasks while live coding, papers related to the data being used, lectures related to the topic of the lab. These prep and in-class activities are closely linked to the assessments to enable skills to develop through practice. We also hold drop in practice sessions twice a week where Graduate Teaching Assistants (GTAs) are present to guide students to prep and in-class activites that will help in assessment completion while taking a hands-off the keyboard approach. The assessment enables the students to demonstrate their competency of the skills learnt in class. However, we take the perspective of skills develop through practice therefore each assessment builds on the work not only completed in the associated lab but provides an opportunity to demomstrate skill acquisition of skills learnt to date over the previous labs. Students will have received the solution for the previously attempted lab as a minimum, grade and feedback as well as the solution if feedback and grade turnaround permits, so this is an excellent opportunity for them to demonstrate integration of feedback and related skill development. Live coding is an important part of our teaching. We structure our classes with a staff member leading with support from two Graduate Teaching Assistants (GTAs). This enables the staff member to lead the class through the content while the GTAs can ensure the individual students are being challenged and supported adequately. The structure of skill building over labs supports this mode of teaching with the focus being on what each component of the code enables us to do rather than memorising code. Additional lab activites, such as the building of a portfolio over each semester, aims to encourage our students to focus on the understanding of the aim of the task the obtaining the code rather than memorising code only. The overall goal for our students in our year 1 Psychology labs *To understand the current state of psychological science and what Open Science is as well as its importance *To be clear on the importance of being confident and competent in data management *To be confident and competent at using RStudio as a tool to acheive good data management skills "],
["lab-1.html", "Chapter 1 Lab 1 1.1 Pre-class activity 1.2 In-class activity 1.3 Post-class activity 1.4 Additional resources around open science and replicability", " Chapter 1 Lab 1 In this lab we will be introduced to the skills we will develop over the next year and beyond. This lab will introduce Open Science and why it is relevant to your development as a Psychologist. We will meet our groups and the staff that will be leading our labs and who we will get to know well over these two semesters. We can't wait to get started! 1.1 Pre-class activity Watch this video of Dr Simine Vazire talking about Open Science and why it is an important concept for us to understand in Psychology. Intro to open science 1.2 In-class activity We wanted to give students a little more context about the importance of taking an open and replicable position with their research so provided a simple drawing task where each of them had to follow the instructions below. The output is varied as our staff attempts show! This was lab 1 so the students had not been introduced to R yet but the issues and debate was being presented. Replication Task : In your groups please complete these steps on a bit of paper. Draw a square Draw another square at 15 degree angle to the bottom of the 1st square Draw 4 lines down from each corner of the 2nd square Draw a circle next to 2nd square Draw a smaller circle on top of first circle Draw upward line from side of 1st circle *Draw 2 triangles on top of 2nd circle Two examples of different interpretations of task: This is the data we will use this semester to develop our skills and knowledge. Familiarise yourself with the data and the study it comes from. Dataset is from Woodworth et al. (2018) details of which can be found here Web-based Positive Psychology Interventions: A Reexamination of Effectiveness 1.3 Post-class activity Formative task where the students have to develop a research question relative to the data and a plan of the steps needed to wrangle the data. RStudio has not been introduced to them yet but open science and replicability has. The aim of this exercise is for students to work with real data which is big and messy so they realise the importance of having an effective tool for handling data. 1.4 Additional resources around open science and replicability Is most published research wrong? Replicability and Reproducibility Debate with Professor Dorothy Bishop Why an Entire Field of Psychology Is in Trouble "],
["lab-2.html", "Chapter 2 Lab 2 2.1 Pre-class activity 2.2 In-class activity", " Chapter 2 Lab 2 This is the first lab that the students are introduced to R. The important point here is that R is introduced as a tool which will support their open science practices such as replicability. 2.1 Pre-class activity 2.1.1 Data Management Watch video on intro to RStudio Remember these key points as you work through the data activities on this course RStudio is a tool that enables you to become confident and competent working with data This is essential as a psychologist as you need to know the data findings are based on are reliable and valid. Imagine delivering a treatment that was based on research where the analyst was not confident or competent? See this tweet from Dr Dale Barr on how applicable these skills will be for you in your future careers These are skills that you will use, and that are desireable to employers. Just ask our previous students: Really glad I did my undergrad psychology degree at Glasgow. Finding writing this PhD protocol really tough but have realised that I was instilled with values of transparent science (which combined with my SU/Survivor researcher hat) gives me determination to keep at it. — Steph NicAllan ((???)) July 5, 2018 Think of a skill you nailed the first time you did it. Not so easy? You will learn to work with data by repeating the essential basic skills over and over. It's new and will take time. Give yourself credit for trying and don't stress if it doesn't work right away. Use the resources we give, that are being talked about online among the open science community and ask questions in class and on the forums. We are here to help! 2.1.2 The first thing we need to do is set the working directory. What this means is that we tell R, where the files we need are. Think of it just like when you have different subjects, and you have seperate folders for each topic e.g. biology, history and so on. When working on R, it's useful to have the data sets and files in one folder. To set working directory press session -&gt; set working directory -&gt; choose directory and then select the folder where the data sets we are working on are saved, and save this file in the same folder as well. In other words- make sure your data sets and scripts are all in the same folder. Follow the pre-lab video to help you with this too. In the labs, we recommend that you create a folder for Psychology labs with sub-folders within for each lab in your M: drive. This is your personal area on the University network that is safe and secure so much better than flashdrives and desktops. 2.1.3 Important info on our homework worksheets So we can see your progress in data management skills we are using a worksheet format for the homework only called RMarkdown (abbreviated as Rmd) which is a great way to create dynamic documents with embedded chunks of code. These documents are self-contained and fully reproducible which makes it very easy to share. This is an important part of your open science training as one of the reasons we are using RStudio is that it enables us to share open and reproducible information. The reason we are using these worksheets in this format is so that you are given a task and then can fill in the required code. For more information about RMarkdown feel free to have a look at their main webpage sometime http://rmarkdown.rstudio.com. The key advantage of RMarkdown is that it allows you to write code into a document, along with regular text, and then 'knit' it using the package knitr to create your document as either a webpage (HTML), a PDF, or Word document (.docx). Before submitting your homework file check that it knits OK to html then submit the .Rmd file, not the knitted html version You won't be creating webpages or other document types (unless you want to!) just yet but we want to tell you a little more about the format of these worksheets so you know how and why we are using them. It enables you to enter code after class for assessment. We can then use a program to mark these and return feedback to you. This is right in line with our ethos of open and reproducible science enabling us to show you a skill, let you try it and then enable you to show us how good you are at it! There are just a couple of important rules we need you to follow to make sure this all runs smoothly. These worksheets need to you fill in your answers and not change any other information. For example, if we ask you to replace NULL with your answer only write in the code you are giving as your answer and nothing else. To illustrate - 2.1.3.0.1 Task 1 read in your data data &lt;- NULL The task above is to read in the data file we are using for this task - the correct answer is data &lt;- read_csv(data.csv). You would replace the NULL with: 2.1.3.0.2 Solution to Task 1 data &lt;- read_csv(&quot;data.csv&quot;) This means we can look for your code and if it is in the format we expect to see it we can give you the marks! If you decide to get all creative on us then we can't give you the marks as 'my_lab_Nov_2018.csv' isn't the filename we have given to you to use. So don't change the file, variable or data frame names as we need these to be consistent. We will look for your answers within the boxes which start and end with ``` and have {r task name} in them e.g. ```{r chunk-name, messages=FALSE} library(tidyverse) ``` These are called code chunks and are the part of the worksheet that we can read and pick out your answers. If you change these in any way we can't read your answer and therefore give you grades. You can see in the example above that the code chunk, grey zone, starts and ends with these back ticks (usually found on top left corner of the keyboard). This code chunk has the ticks and text which makes it the part of the worksheet that will contain code. The {r tidyverse} part tells us which task it is (loading in tidyverse) and therefore what we should be looking for and what we can give marks for - loading in the package called tidyverse in the example above. If this changes then it won't be read properly so will impact on your grade. 2.1.4 Take home message The easiest way to use our worksheets is to think of them as fill-in-the-blanks and keep file names and names used in the worksheet the same. If you are unsure about anything then use the forums on Moodle to ask any questions and come along to the homework sessions. We have also made a short video about R Markdown and why we use it for our lab work which you can find in the prep section of lab 2 on Moodle. There is an R Markdown cheatsheet which will support you playing around with R Markdown which you can find here https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf. 2.2 In-class activity Part of becoming a psychologist is asking questions and gathering data to enable you to answer these questions effectively. It is very important that we understand all aspects of the research process such as experimental design, ethics, data management and visualisation. In this class, you will be learning how to develop reproducible scripts. This means scripts that completely and transparently perform some analysis from start to finish in a way that yields the same result for different people using the same software on different computers. And transparency is a key value of science, as embodied in the “trust but verify” motto. When you do things reproducibly, others can understand and check your work. This benefits science, but there is a selfish reason, too: the most important person who will benefit from a reproducible script is your future self. When you return to an analysis after two weeks of vacation, you will thank your earlier self for doing things in a transparent, reproducible way, as you can easily pick up right where you left off. This is relevant within the context of open science which is a big debate in the scientific community at the moment. Some classic psychological experiments have been found to not be replicable. See the links below for some discussion on this topic: Study delivers bleak verdict on validity of psychology experiment results Low replicability in psychological science As part of your skill development, it is very important that you work with data so you can become confident and competent in your management and analysis of data. In the labs, we will work with real data that we will find from databases. When we are working with data we will use software called RStudio . Information about this software can be found here RStudio. R Studio is available on the computers in the lab but is free to download onto your own devices. Guidance on how to do this can be found RStudio download. More information on why we are working with RStudio is here and the link to the prep video on RStudio you are to watch for this lab can be found here. Getting our data ready to work with Today in the lab we will be working on loading libraries (opening the apps) required to work with our data and then loading our data into RStudio beofre getting it organised into a sensible format that relates to our research question. OK, so what is our first step? Yup, thats right, we need to tell RStudio what packages we want to use and where to find all the files we need before we pull them in ready for work. Task 1 - loading in the package library(tidyverse) Task 2 - read in data dat &lt;- read_csv (&#39;files/ahi-cesd.csv&#39;) pinfo &lt;- read_csv(&#39;files/participant-info.csv&#39;) Task 3 - join the files together all_dat &lt;- inner_join(dat, pinfo, by=&#39;id&#39;, &#39;intervention&#39;) Task 4 - pull out variables of interest summarydata &lt;- select(all_dat, ahiTotal, cesdTotal, sex, age, educ, income, occasion, elapsed.days) Well done! You have started on your journey to become a confident and competent member of the open scientific community! To show us how competent you are you should now complete the assessment for this lab which follows the same instructions as this in-class activity but asks you to work with different variables. Always us the lab prep materials as well as what you do in class to help you complete the class assessments! Success through repetition! Upload you homework assessment which can be found here by 12pm 1 week from your lab. The assessment submission page can be found here. "],
["lab-3.html", "Chapter 3 Lab 3 3.1 Pre-class activity 3.2 In-class activity", " Chapter 3 Lab 3 In this lab we move on from reading data in and joining them together before selecting variables of interest to becoming familiar with the Wickham 6 and the functionality of tidyverse. Practice is an important part of PsyTeachR so in our level 1 program we ask students to repeat tasks over the labs and between in-class and assessments. This principle is carried over into the student's independent study practices as we encourage students to do prep independently but also come along to the practice sessions where space is open for students to work on lab tasks and assessments independently or discuss in groups but GTAs are present to guide and support if needed. This is an important part of our community building and creating a safe space to practice, make mistakes and develop skills. 3.1 Pre-class activity Watch Heather's video on intro to tidyverse Heather's tidyverse video Work through the tidyverse book which talks you through the purpose of tidyverse using examples on a dataset called babynames. This package is installed on the lab computers but you can also install.packages(&quot;babynames&quot;) on your own personal computer if you want to prep for the lab outside the labs. Tidyverse (https://www.tidyverse.org/) is a collection of R packages created by world-famous data scientist Hadley Wickham. Tidyverse contains six core packages: dplyr , tidyr , readr , purrr , ggplot2 , and tibble. If you've ever typed library(tidyverse) into R, you will have seen that it loads in all of these packages in one go. Within these six core packages, you should be able to find everything you need to analyse your data. In this lab, we are going to focus on the dplyr package, which contains six important functions: select() Include or exclude certain variables (columns) filter() Include or exclude certain observations (rows) mutate() Create new variables (columns) arrange() Change the order of observations (rows) group_by() Organize the observations into groups summarise() Derive aggregate variables for groups of observations These six functions are known as ’single table verbs’ because they only operate on one table at a time. Later in the course you will learn two-table verbs that you can use to merge tables together. Although the operations of these functions may seem very simplistic, it’s amazing what you can accomplish when you string them together: Hadley Wickham has claimed that 90% of data analysis can be reduced to the operations described by these six functions. Remember these key points as you work through the data activities on this course RStudio is a tool that enables you to become confident and competent working with data This is essential as a psychologist as you need to know the data findings are based on are reliable and valid. Imagine delivering a treatment that was based on research where the analyst was not confident or competent? Think of a skill you nailed the first time you did it. Not so easy? You will learn to work with data by repeating these basic skills over and over. It's new and will take time. Give yourself credit for trying and don't stress if it doesn't work right away. Use the resources we give, that are being talked about online among the open science community and ask questions in class and on the forums. 3.2 In-class activity OK, so what is our first step? Yup, thats right, we need to tell RStudio where to find all the files we need and pull them in ready for work. Let's go back to lab 2 and use our previous script to remember how to load the package, read in the data and join the 2 data files together: Task 1: Open up tidyverse and read in data library(tidyverse) dat &lt;- read_csv (&#39;files/ahi-cesd.csv&#39;) pinfo &lt;- read_csv(&#39;files/participant-info.csv&#39;) all_dat &lt;- inner_join(dat, pinfo, by=&#39;id&#39;, &#39;intervention&#39;) Now lets get busy using our tidyverse verbs... Task 2. Select the columns all_dat, ahiTotal, cesdTotal, sex, age, educ, income, occasion, elapsed.days from the data and creat variable summarydata. summarydata &lt;- select(all_dat, ahiTotal, cesdTotal, sex, age, educ, income, occasion, elapsed.days) Task 3. Arrange the data in the variable created above (summarydata) by ahiTotal with lowest score first. ahi_asc &lt;- arrange(summarydata, by = ahiTotal) Task 4. Filter the data ahi_desc by taking out those who are over 65 years of age. age_65max &lt;- filter(ahi_asc, age &lt; 65) Task 5. . Then, use summarise to create a new variable data_median, which calculates the median ahiTotal score in this grouped data and assign it a table head called median_score. (Hint: if you're stuck, see .......). data_median &lt;- summarise(age_65max, median_score = median(ahiTotal)) Task 6. Group the data stored in the variable age_65max by sex, and store it in data_sex then use mutate to create a new column called Happiness which categorises participants based on whether they score above the median ahiTotal score for females. data_sex &lt;- group_by(age_65max, sex) happy_female &lt;- mutate(data_sex, Happiness_Female = (ahiTotal &gt; 74)) "],
["lab-4.html", "Chapter 4 Lab 4 4.1 Pre-class activity 4.2 In-class activity 4.3 Post-class activity 4.4 Additional resources", " Chapter 4 Lab 4 4.1 Pre-class activity Interesting twitter conversation between Prof Lisa Debruine and Dr Lisa Debruine on the importance of visualising data which you will be learning about in this lab 4.2 In-class activity 4.3 Post-class activity 4.4 Additional resources "],
["lab-5.html", "Chapter 5 Lab 5 5.1 Pre-class activity 5.2 Wrangling Data 5.3 The Autism Quotient (AQ) 5.4 Thinking through the problem 5.5 In class activity 5.6 Making the computer do the dirty work", " Chapter 5 Lab 5 5.1 Pre-class activity 5.2 Wrangling Data It would be nice to always get data formatted in the way that you want it, but alas, one of the challenges as a scientist is dealing with Other People’s Data. People often structure data in ways that is convenient for data entry, but not very convenient for data analysis, and so much effort must be expended ’wrangling’ data into shape before you can do more interesting things with it. Also, performing analyses often requires pulling together data obtained from different sources: you have done this in semester 1 by combining the participant information with the depression and happiness data. In this lesson, we are going to give you some tips on how to structure data, and introduce strategies for transforming and combining data from different sources. 5.3 The Autism Quotient (AQ) To illustrate these concepts, we will use some actual survey data that was collected online using SurveyMonkey.com (Note: the data have been slightly cleaned up to make things a bit simpler; in reality, the data files were even messier than it appears!) For this research project, prospective participants completed the short 10-item version of the Autism-Spectrum Quotient (AQ) (Baron-Cohen, Wheelwright, Skinner, Martin, &amp; Clubley, 2001), which is designed to measure autistic traits in adults. The data from Survey Monkey was downloaded as a .csv file, a common text format that you should be familiar with by now. Table 1: The ten items on the AQ-10. Q No. Question Q1 I often notice small sounds when others do not. Q2 I usually concentrate more on the whole picture, rather than small details. Q3 I find it easy to do more than one thing at once. Q4 If there is an interruption, I can switch back to what I was doing very quickly. Q5 I find it easy to read between the lines when someone is talking to me. Q6 I know how to tell if someone listening to me is getting bored. Q7 When I’m reading a story, I find it difficult to work out the characters’ intentions. Q8 I like to collect information about categories of things. Q9 I find it easy to work out what someone is thinking or feeling just by looking at their face. Q10 I find it difficult to work out people’s intentions. Responses to each item were measured on a four-point scale: Definitely Disagree, Slightly Disagree, Slightly Agree, Definitely Agree. To avoid response bias, each question is scored according to one of two different question formats. For questions 1, 7, 8, and 10, a point is assigned for agreement (either “Slightly Agree” or “Definitely Agree”) and zero otherwise. We will call this format “F” (for forward). The remaining questions are reverse coded (format “R”): a point is assigned for disagreement (“Slightly Disagree” or “Definitely Disagree”) and zero otherwise. The AQ for each participant is just the total points across all 10 questions. The higher the AQ score, the more ’autistic traits’ they are assumed to exhibit. Our goal is to calculate an AQ score for each participant in the dataset. Download the data here: AQ Data The data should look like this: To preserve anonymity, each participant was given a unique number, coded by the variable Id, shown in the first column of the table. The rest of the columns contain the responses associated with that participant for each of the 10 question (Q1, Q2, Q3, …, Q10). This data format is known as wide format: each unit of analysis (participant) forms a single row in the table, and each observation on that unit forms a separate column. Wide format is often convenient for data entry, but for reasons that will soon become apparent, we will find this format inconvenient for the task of scoring the responses. 5.4 Thinking through the problem How would you go about calculating AQ scores for each participant? Of course, you could do this by hand, but you have 10 responses for each of 66 participants. That makes 660 responses to code by hand. Not only is this a horribly mind-numbing task, it is also one in which you will be prone to make errors, especially as you get bored and your mind begins to wander. Even if you are 99% accurate, that means you will still get about 7 of the scores wrong. Worst of all, this approach does not scale beyond small datasets. As this survey was done over the internet, it could have easily been the case that you ended up with 6,600 participants instead of just 66. Learn how to make the computer do the mind-numbing tasks; it won’t make mistakes, and will free up your mind to focus on the bigger issues in your research. These investments in building your data science skills will pay off handsomely in the long run as these are skills we as researchers use everyday. OK, let’s imagine we are doing the task by hand so that we understand the logic. Once that logic is clear, we’ll go through it again and show you how to write the script to make it happen. Let’s take stock of what we know. First, we know that there are two question formats, and that questions Q1, Q7, Q8, and Q10 are scored according to format F and questions Q2, Q3, Q4, Q5, Q6, and Q9 are scored according to format R. We can represent this information in the table below: Question QFormat Q1 F Q2 R Q3 R Q4 R Q5 R Q6 R Q7 F Q8 F Q9 R Q10 F We also know that for format F, we award a point for agree, zero for disagree; and for format R, a point for disagree, zero for agree. We can represent that information as: QFormat Response Score F Definitely Agree 1 F Slightly Agree 1 F Slightly Disagree 0 F Definitely Disagree 0 R Definitely Agree 0 R Slightly Agree 0 R Slightly Disagree 1 R Definitely Disagree 1 Now that we have put that information into tables, it just becomes a simple matter of looking up the format based on the question number (Q1, Q2, …, Q10); given the format and the response, we can then assign the score. Let’s walk through the example with the first participant. For this participant (Id = 16), we have the following responses: Question Response Q1 Slightly Disagree Q2 Definitely Agree Q3 Slightly Disagree Q4 Definitely Disagree Q5 Slightly Agree Q6 Slightly Agree Q7 Slightly Agree Q8 Definitely Disagree Q9 Slightly Agree Q10 Slightly Agree Note that we have re-formatted the responses so that each response is in a separate row, rather than having all of the responses in a single row, as above. We have reshaped the data from its original wide format to long format. Obviously, this format is called ’long’ because instead of having just one row for each participant, we will now have 10 rows for a total of 66 x 10 = 660. While this format makes it less easy to take the whole dataset in with a single glance, it actually ends up being much easier to deal with, because ’Question’ is a now a single variable whose levels are Q1, Q2, …, Q10, and ’Response’ is also now a single variable. Most functions that you will be working with in R will expect your data to be in long rather than wide format. Let’s now look up the format for each question: Question Response QFormat Q1 Slightly Disagree F Q2 Definitely Agree R Q3 Slightly Disagree R Q4 Definitely Disagree R Q5 Slightly Agree R Q6 Slightly Agree R Q7 Slightly Agree F Q8 Definitely Disagree F Q9 Slightly Agree R Q10 Slightly Agree F And now that we have the format and the response, we can look up the scores: Question Response QFormat Score Q1 Slightly Disagree F 0 Q2 Definitely Agree R 0 Q3 Slightly Disagree R 1 Q4 Definitely Disagree R 1 Q5 Slightly Agree R 0 Q6 Slightly Agree R 0 Q7 Slightly Agree F 1 Q8 Definitely Disagree F 0 Q9 Slightly Agree R 0 Q10 Slightly Agree F 1 Then we just add up the scores, which yields an AQ score of 4 for participant 16. We would then repeat this logic for the remaining 65 participants.Anyone fancy doing this for a big data set?! We will work on using R to get our data into shape in lab 1 and introduce pipes in lab 2 which are a more efficient way of joining functions together. 5.5 In class activity 5.6 Making the computer do the dirty work Now let’s continue what we started in the prep by hand but now using R to change the data from wide to long format. First, we will need to create a new working directory and download the data files. responses.csv : survey responses qformats.csv : table linking questions to question formats scoring.csv : table linking formats and responses to score pinfo.csv : participant information (Sex, Age, etc.) Make a new R script, and make sure to set your working directory where those files can be located and load in tidyverse at the top of your R script then load in the three first files. responses &lt;- read_csv(&quot;files/responses.csv&quot;) # survey responses qformats &lt;- read_csv(&quot;files/qformats.csv&quot;) # question formats scoring &lt;- read_csv(&quot;files/scoring.csv&quot;) # scoring info Whenever you load in data, it is always a good idea to have a quick look to make sure things make sense (using glimpse(), View() or just clicking on them in the environment). For example, we can view the responses table by just typing the name responses in the console. Do that now. Now we can use R to transform our data from wide to long. We will use the function gather() function but we will do it for one participant just now. Some info on this gather() function can be found here What does gather() do?. rlong &lt;- gather(responses, Question, Response, Q1:Q10) What this means is that we have created a tibble called rlong using the function gather. We want gather to look in a previously craeted tibble called responses and then pull out the variables 'Question' and 'Response' for questions 1 to 10. Q1:Q10 is just a convenient shorthand for Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q10. We have now created a tibble with 660 observations over 3 variables; 10 observations per 66 participants on 3 variables. Let's make it simpler just now and only use one participant. We can do that by using filter() which you used last semester and creating a new tibble called rlong_16. rlong_16 &lt;- filter(rlong, Id == 16) We should now have a tibble with 10 observations of 3 variables. OK, the next step: match each question with a format F or R. We have this information in the table qformats. So how do we combine these two tables? We want to match up the tables on the column Question. We can do this using an inner_join(). Try it and see what happens: rlong2 &lt;- inner_join(rlong_16, qformats, &quot;Question&quot;) Magic! The inner_join(), in effect added a new column to rlong which had the format of each question. Let’s look at the function call again.... The first two arguments to inner_join(), rlong and qformats, are the names of the tibbles that we want to join together. The next argument tells R how to join them, by Question. What the inner join does is match up rows in the two tables where both tables have the same value for the field named in the third argument, “Question”; it then combines the columns from the two tables, copying rows where necessary. To state it more simply, what it does, in effect, is the following: For each row in rlong, it checks the value of the column Question, and looks for rows with the same value in qformats, and then essentially combines all of the other columns in the two tables for these matching rows. If there are unmatching values, the rows get dropped. The inner_join() is one of the most useful and time-saving operations in data wrangling so keep ptracticing as it will keep reappearing time after time. Now that we have matched up each question with its corresponding format, we can now “look up” the corresponding scores in the scoring table based on the format and the response. So what we need to do is to combine information in our new table, rlong2, and the scoring table. We do this, once again, with an inner join, and we will store the result in a new variable, rscores. rscores &lt;- inner_join(rlong2, scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) So here we have created a new tibble called rscores using inner_join to join rlong2 and scoring using columns QFormat and Response and you are at a point where you are ready to calculate the AQ scores for participant 16. You have used the function to calculate the total last semester. Can you think where? Look back in your scripts and .Rmd files to remember where you have used this and how you can use it to calculate the total for participant 16. "],
["lab-6.html", "Chapter 6 Lab 6 6.1 Pre-class activity 6.2 Inclass activity", " Chapter 6 Lab 6 6.1 Pre-class activity Students were directed to labs 4 and 5 to recap on skills needed before applying them to this data set introduced in lab 5. 6.2 Inclass activity 6.2.1 Task 1 - loading in the package We are going to be working with the Autism Quotient (AQ) data which you have already been introduced to in lab 1 and in the pre-class for this lab. The relevant data required to use this data comes in 3 seperate files; the survey responses 'responses.csv', the question formats 'qformats.csv' and scoring info 'scoring.csv'. To get us started, load in tidyverse. # load in tidyverse by replacing this line library(tidyverse) 6.2.2 Task 2 - read in data Read in the data to create responses, qformats and scoring using the corresponding data files for each tibble i.e. responses for the response data, qformats for the question formats and scoring for the scoring info. responses &lt;- read_csv(&quot;files/responses.csv&quot;) qformats &lt;- read_csv(&quot;files/qformats.csv&quot;) scoring &lt;- read_csv(&quot;files/scoring.csv&quot;) Checkpoint Has the data now appeared in the environment? Take a look and make sure they look as you would expect them to. Do they look the same as they did in the .csv files? Is it in the format ideal for analysis (see lab 1 in-class and lab 2 prep for a clue)? 6.2.3 Task 3 - getting our data into the correct format for analysis Let's get this data into long format from wide format.Firstly, create rlong by using gather to show the question and response for each participant in long format. rlong &lt;- gather(responses, Question, Response, Q1:Q10) 6.2.4 Task 4 - Question format Now use inner_join to add in the question format (QFormat) for each question and store it in rlong2. This should now have 4 columns showing Id, Question, Response and QFormat. rlong2 &lt;- inner_join(rlong, qformats, &quot;Question&quot;) 6.2.5 Task 5 - Scores We can now get the score for each participant for each question. Use inner_join to add in the score for each question and store it in rscores. rscores &lt;- inner_join(rlong2, scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) 6.2.6 Task 6 - Calculate AQ scores To calculate the AQ score for each participant, firstly we need to group the scores by Id. Using the tibble rscores defined in task 5, group the AQ scores by Id and store it in aq_scores1. aq_scores1 &lt;- group_by(rscores, Id) 6.2.7 Task 7 - Calculate AQ scores cont'd Now we need we have the scores grouped by Id we can use summarise to calculate the sum of each question's score for each participant. Do this for aq_scores1 and store it in aq_scores2. aq_scores2 &lt;- summarise(aq_scores1, AQ = sum(Score)) 6.2.8 Task 8 - Calculate AQ scores efficiently There is a more efficient way of calculating the AQ scores for each participant by combining the lines of code in tasks 6, 7 and 8 together into one tibble using pipes. Using the tibble rscores defined in task 5, calculate AQ scores for each participant. Name this table aq_scores (USE THIS EXACT NAME!) aq_scores &lt;- rscores %&gt;% group_by(Id) %&gt;% summarise(AQ = sum(Score)) Checkpoint Does aq_scores look the same as aq-scores2? 6.2.9 Task 9 - Using ggplot2, make a distribution of the overall AQ scores by producing a histogram from aq_scores. ## TODO: uncomment the following line, and replace it with your answer: ggplot(aq_scores, aes(AQ)) + geom_histogram() Figure 6.1: CAPTION THIS FIGURE!! 6.2.9.1 Well done! You should use this activity to help you complete the assessment to be submitted 1 week from your lab by 12pm. "],
["lab-7.html", "Chapter 7 Lab 7 7.1 Preclass activity 7.2 Probability and Probability Distributions 7.3 Theoretical probability distributions 7.4 Inclass activity 7.5 Monte Carlo simulations 7.6 The Binomial Distribution - Creating a Discrete Distribution", " Chapter 7 Lab 7 7.1 Preclass activity The activity below was supported by a short video by Heather. 7.2 Probability and Probability Distributions The aim of Lab 3 is to introduce concepts and R/RStudio functions that you can use to: calculate probabilities create probability distributions make estimations from probability distributions. There are no cheatsheets for this lab as we will not be using a specific package. However you can make full use of the R help function (e.g. ?sample) when you are not clear on what a function does. There are loads of videos and help pages out there with clear examples to explain difficult concepts. For now though, and to start this prep, open a new script and load in the tidyverse package (we need this for some visualisation work later). library(&quot;tidyverse&quot;) 7.2.1 Types of data How you tackle probability depends on the type of data/variables you are working with (i.e. discrete or continuous). Sometimes also referred to as Level of Measurements. Discrete data can only take integer values (whole numbers). For example, the number of participants in an experiment would be discrete - we can't have half a participant! Discrete variables can also be further broken down into nominal and ordinal variables. Ordinal data is a set of ordered categories; you know which is the top/best and which is the worst/lowest, but not the difference between categories. For example, you could ask participants to rate the attractiveness of different faces based on a 5-item Likert scale (very unattractive, unattractive, neutral, attractive, very attractive). Nominal data is also based on a set of categories but the ordering doesn't matter (e.g. left or right handed). Nominal is sometimes simply refered to as categorical data. Continuous data on the other hand can take any value. For example, we can measure age on a continuous scale (e.g. we can have an age of 26.55 years), also reaction time or the distance you travel to university every day. It can be broken into interval or ratio data; but more of that another time. 7.2.2 Simple probability calculations Today, we are going to run you through the concepts of probability calculations. Probability is the extent to which an event is likely to occur and is represented by a real number p between 0 and 1. For example, the probability of flipping a coin and it landing on 'tails' most people would say is estimated at p = .5. Calculating the probability of any discrete event occuring can be formulated as: \\[p = \\frac{number \\ of \\ ways \\ the \\ event \\ could \\ arise}{number \\ of \\ possible \\ outcomes}\\] For example, what is the probability of randomly drawing your name out of a hat of 12 names where one name is definitely yours? 1/12 ## [1] 0.08333333 7.2.3 Joint probability Describing the probability of single events, such as a coin flip or rolling a six is easy, but more often than not we are interested in the probability of a collection of events, such as the number of heads out of 10 flips or the number of sixes in 100 rolls. For simplicity, let’s take the example of a coin flip. Let’s say we flip a coin four times. Assuming the coin is fair (probability of heads = .5), we can ask question such as: what is the probability that we get heads on exactly one out of the four coin flips? 7.2.4 Prep exercise: Estimate the probability of getting X heads over four independent flips. Let’s start by introducing the sample() function in R, which samples elements from a vector x. A vector is a sequence of data elements of the same basic type. Members in a vector are officially called components. Let’s define a vector c(&quot;HEADS&quot;, &quot;TAILS&quot;) and take four samples from it. # Notice that because our event labels are strings (text), # we need to enter them into the function as a vector; i.e. in &quot;quotes&quot; sample(c(&quot;HEADS&quot;, &quot;TAILS&quot;), size = 4, replace = TRUE) ## [1] &quot;HEADS&quot; &quot;HEADS&quot; &quot;HEADS&quot; &quot;HEADS&quot; What you have done here is tell R that you want to sample observations, which in this case was c(&quot;HEADS&quot;, &quot;TAILS&quot;) and the number of samples you want to take, we chose four, which is larger than the size of the vector itself (it has only two components; HEADS and TAILS). For this reason we need to override the default of sampling without replacement or we will get an error (we set replace to TRUE). Sampling without replacement means once an observation appears in the sample, it is removed from the source vector so that it can’t be drawn again. Sampling with replacement means that an observation is retained so it can't be drawn again. In this case that would mean not being able to toss the coin and get HEADS again after tossing it once which is impossible on a coin. It will make our lives easier if we define heads as one and tails as zero and write the same command as follows: # Now that our event labels are numeric, we don&#39;t need the vector. # 0:1 means all numbers from 0 to 1 in steps of 1. So basically, 0 and 1. sample(0:1, 4, TRUE) ## [1] 0 0 1 1 This is logically equivalent, but using ones and zeroes means that we can count the number of heads by simply summing up the values in the vector. For instance: # The above statement pipes the output of sample() #then the function sum() which counts up the number of heads. sample(0:1, 4, TRUE) %&gt;% sum() ## [1] 2 Run this function multiple times (you can use the up arrow in the console to avoid having to type it over and over). I ran it five times, and got this: The results of my little (N = 5) simulation of flipping four coins and counting heads were: 1, 3, 2, 2, and 3. So in only one out of the five simulations did I get exactly one heads. Let’s repeat the experiment a whole bunch more times. We can have R do this over and over again using the replicate() function from base R. The basic syntax of replicate() is replicate(number_of_times, expression_you_want_to_replicate). So # repeats the expression sample(0:1, 4, TRUE) %&gt;% sum() 20 times; the result comes out as a vector with each number represents a count of the number of heads over four flips in each of the 20 ’experiments’. replicate(20, sample(0:1, 4, TRUE) %&gt;% sum()) ## [1] 0 3 4 3 2 1 4 4 1 2 3 2 3 4 2 1 2 2 3 2 7.2.5 Monte Carlo simulation Every year, the city of Monte Carlo is the site of innumerable games of chance played by people from all over the world. This notoriety is reflected in the use of the term “Monte Carlo simulation” among statisticians to refer to use of computer simulation to estimate statistical properties of a random process. In a Monte Carlo simulation, the random process is repeated over and over again in order to assess its performance over the long run. It is usually used in situations where mathematical solutions are unknown or hard to compute. Now we are ready to use Monte Carlo simulation to attack the question of the probability of various outcomes. So we are going to run our coin flip experiment we started in the prep materials but this time we are going to run the experiment 50 times (each including 4 coin tosses), or collect 50 data points, and use the same principles to predict the number of heads we will get. Store the result in a variable heads50 using the code below: heads50 &lt;- replicate(50, sample(0:1, 4, TRUE) %&gt;% sum()) # this time we save the output in the variable heads50 # so we can work with the results further Each of these numbers represents the number of heads as the outcome of 50 lots of 4 coin tosses so the first 2 data points tell us that they each produced heads twice. Your results may differ, since sample() is random. In this case, four of the experiments yielded the outcome of 4/4 heads. Note that we can estimate the probability of each of the outcomes (0, 1, 2, 3, 4 heads) by counting them up and dividing through by the number of experiments. We will do this by putting the experiments in a data_frame() and then using count(). data50 &lt;- data_frame(heads = heads50) %&gt;% # convert to a data frame group_by(heads) %&gt;% # group by number of possibilities summarise(n = n(), p=n/50) # count occurances of possibility, ## Warning: `data_frame()` is deprecated, use `tibble()`. ## This warning is displayed once per session. # &amp; calculate probability (p) of # each What we have done here is created a variable heads50 in a new data_frame object that contains the results from our 50 experiments, grouped it by the values in heads50 (0, 1, 2, 3, or 4), and counted the number of times each value was observed in summarise(n = n()), and then calculated the probability by dividing the newly created variable n by the number of experiments (50). We can then plot a histogram of the outcomes using ggplot2. # Note: stat = &quot;identity&quot; tells ggplot to use the values of the y-axis variable (p) as the height of the bars in our histogram (as opposed counting the number of occurances of those values) ggplot(data50, aes(heads,p)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;purple&quot;) + labs(x = &quot;Number of Heads&quot;, y = &quot;Probability of Heads in 4 flips (p)&quot;) + theme_bw() Figure 7.1: CAPTION THIS FIGURE!! So the estimated probabilites are 0.02 for 0/4 heads, 0.24 for 1/4 heads, etc. Unfortunately sometimes this calculation will estimate that the probabilty of an outcome is zero since this outcome never came up when the simulation is run. If you want reliable estimates, you need a bigger sample as we know that this was a possible outcome but just didnt come about this time. Let's repeat the Monte Carlo simulation, but with 10,000 experiments instead of just 50. Here we are again using R to tackle big data and multiple calculations which is much faster and more efficient than if we had to figure this out by hand! All we have to do is change the 50 to 10000 in the above call to replicate. Again, we'll put the vector in a data frame and calculate counts and probabilities of each outcome using group_by() and summarise(). heads10K &lt;- replicate(10000, sample(0:1, 4, TRUE) %&gt;% sum()) # this time we save the output in the variable heads10K data10K &lt;- data_frame(heads = heads10K) %&gt;% group_by(heads) %&gt;% summarise(n = n(), p=n/10000) ggplot(data10K, aes(heads,p)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;purple&quot;) + labs(x = &quot;Number of Heads&quot;, y = &quot;Probability of Heads in 4 flips (p)&quot;) Figure 7.2: CAPTION THIS FIGURE!! So using Monte Carlo simulation, we estimate that the probability of getting exactly one head on four throws is about 0.25. The above result represents a probability distribution for all the possible outcomes in our experiments. We can derive lots of useful information from this. For instance, what is the probability of getting two or more heads in four throws? This is easy: the outcomes meeting the criterion are 2, 3, or 4 heads. We can just add these probabilities together like so: data10K %&gt;% filter(heads &gt;= 2) %&gt;% summarise(p2 = sum(p)) ## # A tibble: 1 x 1 ## p2 ## &lt;dbl&gt; ## 1 0.692 You can add probabilities for various outcomes together as long as the outcomes are mutually exclusive, that is, when one outcome occurs, the others can't occur. For this coin flipping example, this is obviously the case: you can't simultaneously get exactly two and exactly three heads as the outcome of a single experiment. However, be aware that you can't simply add probabilities together when the events in question are not mutually exclusive: for example, the probability of the coin landing heads up, and it landing with the image of the head being in a particular orientation are not mutually exclusive, and can't be simply added together. The probabilities we got from Monte Carlo simulation are just estimates; we need something more definitive. 7.3 Theoretical probability distributions Recall that probability is not much more sophisticated than counting and dividing. We are now going to move this forward by looking at binomial distributions in lab 3 and normal distributions in lab 4. See you in lab 3! \\[p = \\frac{number \\ of \\ ways \\ the \\ event \\ could \\ arise}{number \\ of \\ possible \\ outcomes}\\] 7.4 Inclass activity 7.5 Monte Carlo simulations Before we move on make sure you are comfortable with Monte Carlo simulation that we learnt about in the prep materials. Remember, the staff and GTAs are in the lab to support your learning so if in doubt just ask to go through it. These are concepts that need some work so take the time get comfortable with them as that will put you in good stead moving forward with this lab and lab 4. 7.6 The Binomial Distribution - Creating a Discrete Distribution So far we have learnt how probabilities and distributions work using the simple probability calculation. However, if we had wanted to calculate the probability of 8 heads from 10 coin flips we don't have to go through this entire procedure each time. Instead, because we have a dichotomous outcome, heads or tails, we can calculate probabilities using the binomial distribution - effectively what you just created. We will walk through some essentials here. We'll use 3 functions to work with the binomial distribution: dbinom - the density function: gives you the probability of x successes (heads) given the no of trials and probability of success probability on a single trial (here it's 0.5, because we assume we're flipping a fair coin). pbinom - the distribution function: gives you the cumulative probability of getting a number of successes below a certain cut-off point (e.g. probability of getting 0 to 5 heads out of 10 flips), given the size and the prob. Known as the cumulative probability distribution function or the cumulative density function. qbinom - the quantile function: is the inverse of pbinom in that it gives you the x axis value below (and including the value) which the summation of probabilities is greater than or equal to a given probability p, plus given the size and prob. So let's try these functions out to answer the two questions: What is the probability of getting exactly 5 heads on 10 flips? What is the probability of getting at most 2 heads on 10 flips? Let's start with question 1, what is the probability of getting exactly 5 heads on 10 flips? So we want to predict the probability of getting 5 heads in 10 trials (coin flips) and the probability of success is 0.5 (it'll either be heads or tails so you have a 50/50 chance which we write as 0.5). We write that as: dbinom(5, 10, 0.5) The probability of getting 5 heads out of 10 coin flips is 0.25 (to 2 decimal places). which would look like this: (#fig:binom_plot1)CAPTION THIS FIGURE!! The dbinom (density binom) function takes the format of dbinom(x, size, prob), where the arguments we give are: * x the number of ‘heads’ we want to know the probability of. Either a single one, 3 or a series 0:10. In this case it's 5. * size the number of trials (flips) we are doing; in this case, 10 flips. * prob the probability of ‘heads’ on one trial. Here chance is 50-50 which as a probability we state as 0.5 or .5 OK, question number 2....What is the probability of getting at most 2 heads on 10 flips? This time we use pbinom as we want to know the cumulative probability of getting a maximum of 2 heads from 10 coin flips so we have set a cut-off point of 2 but we still have a probability of getting a heads of 0.5. pbinom(2, 10, 0.5) The probability of us getting a maximum of 2 heads on 10 coin flips is 0.05 (2 decimal places). Note that there is another way we could have done this: probs &lt;- pbinom(0:2, 10, 0.5) probs sum(probs) Note that in this latter strategy, we are just adding up the heights of the three blue bars for the probabilities of getting 0,1 or 2 heads therefore calculating the cumulative probablity of this scenario. (#fig:binom_plot2)CAPTION THIS FIGURE!! Let's try one more scenario with a cut-off point to make sure we have this. What is the probability of getting 7 or more heads on 10 flips? We can actually approach this in two different ways. We have done one way to answer the previous question which we can use again here but instead of using a maximum of 2 coin flips (0:2) we are setting the cut-off as 7 to 10 (7:10). ## [1] 3.933594 But we can also think about this in terms of the tails of the plots. Are we interested in the lower tail of the plot or the higher tail? Well, it depends on where our cut-off sits. What pbinom() gives us is the probability of getting 0 to 6 successes (the lower tail of the distribution, given by the blue bars). The total area under the curve for a theoretical distribution sums to 1. If we want the upper tail instead, we set lower.tail to FALSE, and this will give us the probability for the blue bars. In this example, our cut off is 7 up to 10 so we will set lower.tail = FALSE as we are interested in the upper tail of the distribution. ## [1] 0.171875 (#fig:binom_plot3)CAPTION THIS FIGURE!! OK now let's consider a scenario in which you'd use the quantile function qbinom(). You suspect that the coin is biased against heads. Your null hypothesis is that the coin is not biased against heads (P(heads)=.5). You are going to run a single experiment to test your hypothesis, with 10 trials. What is the minimum number of successes that is acceptable if you want to keep your chances of getting heads for this type of experiment at .05? ## [1] 2 So if you got less than two heads, you would reject the null that the coin was unbiased against heads. Ten trials is probably far too few. What would your cutoff be if you ran 100 trials? 1000? 10000? ## [1] 42 474 4918 You have used probability in the previous two functions, dbinom and pbinom, and it represents the probability of success on a single trial (here it is the probability of 'heads' in one coin flip, .5). Now, probability represents the probability of success in one trial, whereas p represents the overall probability of success across all trials and gives you the number of heads that would give that probability. So you ask it for the minimum number of successes (e.g. heads) to maintain an overall probability of .05, in 10 flips, when the probability of a success on any one flip is .5. And it tells you the answer is 2. qbinom also uses the lower.tail argument and it works in a similar fashion to pbinom. Congratulations! You are well on you way to understanding binomial probability so complete the assessment for this lab and submit one week from attending by 12pm. Next time we will be looking at normal distributions and probability. "],
["lab-8.html", "Chapter 8 Lab 8 8.1 Pre-class activity 8.2 Discrete or Continuous Datasets 8.3 Normal distribution 8.4 Inclass activity 8.5 Normal Distribution Probability 8.6 Working with the Normal Distribution", " Chapter 8 Lab 8 8.1 Pre-class activity 8.2 Discrete or Continuous Datasets This is a short tutorial with just a couple of quick questions on how the level of measurement can alter the way you tackle probability - i.e. whether the data is discrete or continuous. The four level of measurements are nominal (also called categorical), ordinal, interval and ratio. Discrete data only uses categories or whole numbers and is therefore either nominal or ordinal data. Continuous data can take any value, e.g. 9.00 or 9.999999999, and so is either interval or ratio data. 8.2.0.1 Quickfire Questions Discrete data can only take integer values (whole numbers). For example, the number of participants in an experiment would be discrete - we can't have half a participant! Discrete variables can also be further broken down into nominal/categorical and ordinal variables. Watch this video to help you understand the differences a little more and then try the following questions. link(https://youtu.be/6IdJ1aPFDCs) Fill in the blanks in the below sentences using the words: ordinal, nominal, categorical. data is based on a set of categories but the ordering doesn't matter (e.g. left or right handed. This is sometimes referred to as data. For example, you could separate participants according to left or right handedness or by education level. data is a set of ordered categories; you know which is the top/best and which is the worst/lowest, but not the difference between categories. For example, you could ask participants to rate the attractiveness of different faces based on a 5-item Likert scale (very unattractive, unattractive, neutral, attractive, very attractive). Continuous data on the other hand can take any value. For example, we can measure age on a continuous scale (e.g. we can have an age of 26.55 years), also reaction time or the distance you travel to university every day. Continuous data can be broken into interval or ratio data. Interval data is data which comes in the form of a numerical value where the difference between points is standardised and meaningful. For example temperature, the difference in temperature between 10-20 degrees is the same as the difference in temperature between 20-30 degrees. Ratio data is very like interval but has a true zero point. With our interval temperature example above, we have been experiencing negative temperatures (-1,-2 degrees) in Glasgow but with ratio data you don't see negative values such as these i.e. you can't be -10 cm tall. When you read journal articles or when you are working with data in the lab, it is really good practice to take a minute or two to figure out the type of variables you are reading about and/or working with. To recap, the four level of measurements are nominal (also called categorical), ordinal, interval and ratio. Discrete data only uses categories or whole numbers and is therefore either nominal or ordinal data. Continuous data can take any value, e.g. 9.00 or 9.999999999, and so is either interval or ratio data. We have introduced these concepts in a little more depth as we are moving onto lab 4 and normal distribution probability so this will help with understanding this concept in relation to binomial probability and the different types of data involved. Recap Binomial distributions are based on scenarios where the outcome can only be 1 of 2 options e.g. heads or tails whereas normal distributions are based on continuous data e.g. height or age where you can score anywhere along a continuum. For example, you could describe someone as 21.5 years old and 176cm tall. 8.3 Normal distribution A normal distribution looks like this: Fig. 1 Normal Distribution of height 8.4 Inclass activity 8.5 Normal Distribution Probability In lab 3 and the pre lab for this lab you recapped and expanded on your understanding of probability, including a number of binom functions as well as some more basic ideas on probability and data types. You will need these skills to complete the following so please make sure you have carried out the pre class on types of data before attempting this. Remember to follow the instructions and if you get stuck at any point to ask the staff member and GTAs to talk you through it. Note: You should complete the below code chunks by replacing the NULL (except the library chunk where the appropriate code should just be entered). Staff will talk you through the mean and standard deviations in relation to a normal distribution. A normal distribution looks something like this: !*Fig. 1 Normal Distribution of height With the binomial distribution in lab 3 we plotted the number of successes, heads or tails, we achieved with a certain number of throws so we use a bar plot e.g. how many heads we got between 1 and 10 tosses of the coin. This time with our height example above, we are plotting the number of people of each height along a continuum between two points, ~140cm to ~200cm. For this, we use a histogram which is the plot above. 8.6 Working with the Normal Distribution We won't ask you to create a normal distribution as it is more complicated than the binomial distribution you estimated in lab 3. Unlike coin flips, the outcome in the normal distribution is not just 50/50. Just as with the binomial distribution (and other distributions) there are functions that allow us to estimate the normal distribution and to ask questions about the distribution. These are: dnorm() - the density function pnorm() - the probability or distribution function qnorm() - the quantile function They work in a similar way to their binomial counterparts. If you are unsure about how a function works you can call the help on it by typing in the console, for example, ?dnorm or ?dnorm(), but the brackets aren't essential for the help. In this lab we will focus on pnorm() and qnorm(). You will have become acquainted with these terms in the pre lab materials but let's quickly revist them together before working our way through some other examples. So lets start using some code to calculate the probabilities of people being a certain height. The average height () of a 16-24 year old Scotsman is 176.2 centimetres, with a standard deviation () of 6.748. The average height () of a 16-24 year old Scotswoman is 163.8 cm, with a standard deviation () of 6.931. (Data from the [Scottish Health Survey, 2008] (http://www.scotland.gov.uk/Resource/Doc/286063/0087158.pdf) What is the probability of meeting a 16-24 y.o. Scotswoman who is taller than the average 16-24 y.o. Scotsman? ## NOTE: uncomment and replace NULL with your answer; do not change the name of the variable! p_taller_w &lt;- pnorm(176.2, 163.8, 6.931, lower.tail = FALSE) Fiona is a very tall Scotswoman (181.12cm) in the 16-24 y.o. range who refuses to date men who are shorter than her. What proportion of all 16-24 Scotsmen would she be willing to date? ## NOTE: uncomment and replace NULL with your answer; do not change the name of the variable! p_taller_m &lt;- pnorm(181.12, 176.2, 6.748, lower.tail = FALSE) Out of 100,000 16-24 y.o. Scotswomen, how many would you predict would meet the height eligibility requirement to join the Royal Navy (at least 151.5cm tall)? ## NOTE: uncomment and replace NULL with your answer; do not change the name of the variable! n_sailors &lt;- 100000 * pnorm(151.5, 163.8, 6.931, lower.tail = FALSE) 8.6.1 qnorm() - The Quartile Function Let's use our height example to work with qnorm: How tall would a 16-24 y.o. Scotsman have to be in order to be in the top 5% of the height distribution for Scotsmen in his age group? ## NOTE: uncomment and replace NULL with your answer; do not change the name of the variable! height_cutoff &lt;- qnorm(.05, 176.2, 6.748, lower.tail = FALSE) "]
]
