# Lab 7

## Preclass activity

The activity below was supported by a short video by Heather. 

## Probability and Probability Distributions

The aim of Lab 3 is to introduce concepts and R/RStudio functions that you can use to:

1. calculate probabilities 
2. create probability distributions
3. make estimations from probability distributions. 

There are no cheatsheets for this lab as we will not be using a specific package. However you can make full use of the R help function (e.g. `?sample`) when you are not clear on what a function does. There are loads of videos and help pages out there with clear examples to explain difficult concepts. For now though, and to start this prep, open a new script and load in the `tidyverse` package (we need this for some visualisation work later). 

``` {r libraries, eval = TRUE, message = FALSE}
library("tidyverse")
```

### Types of data

How you tackle probability depends on the type of data/variables you are working with  (i.e. discrete or continuous). Sometimes also referred to as `Level of Measurements`.  

**Discrete** data can only take integer values (whole numbers). For example, the number of  participants in an experiment would be discrete - we can't have half a participant! Discrete variables can also be further broken down into **nominal** and **ordinal** variables. Ordinal data is a set of ordered categories; you know which is the top/best and which is the worst/lowest, but not the difference between categories. For example, you could ask participants to rate the attractiveness of different faces based on a 5-item Likert scale (very unattractive, unattractive, neutral, attractive, very attractive). Nominal data is also based on a set of categories but the ordering doesn't matter (e.g. left or right handed). Nominal is sometimes simply refered to as `categorical` data. 

**Continuous** data on the other hand can take any value. For example, we can measure age on a continuous scale (e.g. we can have an age of 26.55 years), also reaction time or the distance you travel to university every day. It can be broken into **interval** or **ratio** data; but more of that another time.


### Simple probability calculations
Today, we are going to run you through the concepts of probability calculations. 

Probability is the extent to which an event is likely to occur and is represented by a real number *p* between 0 and 1. For example, the probability of flipping a coin and it landing on 'tails' most people would say is estimated at *p = .5*. Calculating the probability  of any discrete event occuring can be formulated as:

$$p = \frac{number \  of  \ ways \ the \ event \ could \  arise}{number \ of \ possible \ outcomes}$$
For example, what is the probability of randomly drawing your name out of a hat of 12 names where one name is definitely yours? 
```{r Q1}
1/12
```


### Joint probability
Describing the probability of single events, such as a coin flip or rolling a six is easy, but more often than not we are interested in the probability of a collection of events, such as the number of heads out of 10 flips or the number of sixes in 100 rolls. For simplicity, let’s take the example of a coin flip.

Let’s say we flip a coin four times. Assuming the coin is fair (probability of heads = .5), we can ask question such as: what is the probability that we get heads on exactly one out of the four coin flips? 

### Prep exercise: Estimate the probability of getting X heads over four independent flips.

Let’s start by introducing the sample() function in R, which samples elements from a vector x. A vector is a sequence of data elements of the same basic type. Members in a vector are officially called components. Let’s define a vector c("HEADS", "TAILS") and take four samples from it.

``` {r coin_flip}
# Notice that because our event labels are strings (text), 
# we need to enter them into the function as a vector; i.e. in "quotes" 
sample(c("HEADS", "TAILS"), size = 4, replace = TRUE) 
```

What you have done here is tell R that you want to sample observations, which in this case was c("HEADS", "TAILS") and the number of samples you want to take, we chose four, which is larger than the size of the vector itself (it has only two components; HEADS and TAILS). For this reason we need to override the default of sampling without replacement or we will get an error (we set replace to TRUE). Sampling without replacement means once an observation appears in the sample, it is removed from the source vector so that it can’t be drawn again. Sampling with replacement means that an observation is retained so it can't be drawn again. In this case that would mean not being able to toss the coin and get HEADS again after tossing it once which is impossible on a coin. 

It will make our lives easier if we define heads as one and tails as zero and write the same command as follows:
``` {r coin_flip_numerical}
# Now that our event labels are numeric, we don't need the vector.
# 0:1 means all numbers from 0 to 1 in steps of 1. So basically, 0 and 1.
sample(0:1, 4, TRUE)
```

This is logically equivalent, but using ones and zeroes means that we can count the number of heads by simply summing up the values in the vector. For instance:
```{r coin_flip_sum}
# The above statement pipes the output of sample() 
#then the function sum() which counts up the number of heads.

sample(0:1, 4, TRUE) %>% sum()
```
 
Run this function multiple times (you can use the up arrow in the console to avoid having to type it over and over). I ran it five times, and got this:

The results of my little (N = 5) simulation of flipping four coins and counting heads were: 1, 3, 2, 2, and 3. So in only one out of the five simulations did I get exactly one heads.

Let’s repeat the experiment a whole bunch more times. We can have R do this over and over again using the replicate() function from base R. The basic syntax of replicate() is replicate(number_of_times, expression_you_want_to_replicate). So

``` {r coin_flip_rep}
# repeats the expression sample(0:1, 4, TRUE) %>% sum() 20 times; the result comes out as a vector with each number represents a count of the number of heads over four flips in each of the 20 ’experiments’.
replicate(20, sample(0:1, 4, TRUE) %>% sum())
```

### Monte Carlo simulation
Every year, the city of Monte Carlo is the site of innumerable games of chance played by people from all over the world. This notoriety is reflected in the use of the term “Monte Carlo simulation” among statisticians to refer to use of computer simulation to estimate statistical properties of a random process. In a Monte Carlo simulation, the random process is repeated over and over again in order to assess its performance over the long run. It is usually used in situations where mathematical solutions are unknown or hard to compute. Now we are ready to use Monte Carlo simulation to attack the question of the probability of various outcomes. 

So we are going to run our coin flip experiment we started in the prep materials but this time we are going to run the experiment 50 times (each including 4 coin tosses), or collect 50 data points, and use the same principles to predict the number of heads we will get.

Store the result in a variable **heads50** using the code below:

``` {r coin_replication}
heads50 <- replicate(50, sample(0:1, 4, TRUE) %>% sum())   
# this time we save the output in the variable heads50
# so we can work with the results further
```

Each of these numbers represents the number of heads as the outcome of 50 lots of 4 coin tosses so the first 2 data points tell us that they each produced heads twice. Your results may differ, since sample() is random. In this case, four of the experiments yielded the outcome of 4/4 heads.

Note that we can estimate the probability of each of the outcomes (0, 1, 2, 3, 4 heads) by counting them up and dividing through by the number of experiments. We will do this by putting the experiments in a data_frame() and then using count().


``` {r coin_distribution}
data50 <- data_frame(heads = heads50) %>%   # convert to a data frame 
                group_by(heads) %>%           # group by number of possibilities
                summarise(n = n(), p=n/50) # count occurances of possibility,
                                              # & calculate probability (p) of
                                              # each
```
  
What we have done here is created a variable **heads50** in a new data_frame object that contains the results from our 50 experiments, grouped it by the values in **heads50** (0, 1, 2, 3, or 4), and counted the number of times each value was observed in summarise(n = n()), and then calculated the probability by dividing the newly created variable n by the number of experiments (50).

We can then plot a histogram of the outcomes using ggplot2.

```{r data50hist}
# Note: stat = "identity" tells  ggplot to use the values of the y-axis variable (p) as the height of the bars in our histogram (as opposed counting the number of occurances of those values)

ggplot(data50, aes(heads,p)) + 
  geom_bar(stat = "identity", fill = "purple") + 
  labs(x = "Number of Heads", y = "Probability of Heads in 4 flips (p)") +
  theme_bw()
```

So the estimated probabilites are 0.02 for 0/4 heads, 0.24 for 1/4 heads, etc. Unfortunately sometimes this calculation will estimate that the probabilty of an outcome is zero since this outcome never came up when the simulation is run. If you want reliable estimates, you need a bigger sample as we know that this was a possible outcome but just didnt come about this time.

Let's repeat the Monte Carlo simulation, but with 10,000 experiments instead of just 50. Here we are again using R to tackle big data and multiple calculations which is much faster and more efficient than if we had to figure this out by hand! 

All we have to do is change the 50 to 10000 in the above call to replicate. Again, we'll put the vector in a data frame and calculate counts and probabilities of each outcome using group_by() and summarise().

``` {r coin_replication_10K}
heads10K <- replicate(10000, sample(0:1, 4, TRUE) %>% sum())   
# this time we save the output in the variable heads10K
```

``` {r coin_distribution_10K}
data10K <- data_frame(heads = heads10K) %>%   
                group_by(heads) %>%           
                summarise(n = n(), p=n/10000) 
```

```{r data10Khist}
ggplot(data10K, aes(heads,p)) + 
  geom_bar(stat = "identity", fill = "purple") + 
  labs(x = "Number of Heads", y = "Probability of Heads in 4 flips (p)") 
```

So using Monte Carlo simulation, we estimate that the probability of getting exactly one head on four throws is about 0.25. The above result represents a probability distribution for all the possible outcomes in our experiments. We can derive lots of useful information from this.

For instance, what is the probability of getting two or more heads in four throws? This is easy: the outcomes meeting the criterion are 2, 3, or 4 heads. We can just add these probabilities together like so:
``` {r data10K}
data10K %>%
  filter(heads >= 2) %>%
  summarise(p2 = sum(p))
```


You can add probabilities for various outcomes together as long as the outcomes are mutually exclusive, that is, when one outcome occurs, the others can't occur. For this coin flipping example, this is obviously the case: you can't simultaneously get exactly two and exactly three heads as the outcome of a single experiment. However, be aware that you can't simply add probabilities together when the events in question are not mutually exclusive: for example, the probability of the coin landing heads up, and it landing with the image of the head being in a particular orientation are not mutually exclusive, and can't be simply added together.

The probabilities we got from Monte Carlo simulation are just estimates; we need something more definitive.

## Theoretical probability distributions
Recall that probability is not much more sophisticated than counting and dividing. We are now going to move this forward by looking at binomial distributions in lab 3 and normal distributions in lab 4. See you in lab 3!


$$p = \frac{number \  of  \ ways \ the \ event \ could \  arise}{number \ of \ possible \ outcomes}$$

## Inclass activity

## Monte Carlo simulations
Before we move on make sure you are comfortable with Monte Carlo simulation that we learnt about in the prep materials. Remember, the staff and GTAs are in the lab to support your learning so if in doubt just ask to go through it. These are concepts that need some work so take the time get comfortable with them as that will put you in good stead moving forward with this lab and lab 4. 

## The Binomial Distribution - Creating a Discrete Distribution
So far we have learnt how probabilities and distributions work using the simple probability calculation. However, if we had wanted to calculate the probability of 8 heads from 10 coin flips we don't have to go through this entire procedure each time. Instead, because we have a dichotomous outcome, heads or tails, we can calculate probabilities using the **binomial distribution** - effectively what you just created. We will walk through some essentials here.

We'll use 3 functions to work with the binomial distribution:

**dbinom** - the density function: gives you the probability of x successes (heads) given the no of trials and probability of success probability on a single trial (here it's 0.5, because we assume we're flipping a fair coin).

**pbinom** - the distribution function: gives you the cumulative probability of getting a number of successes below a certain cut-off point (e.g. probability of getting 0 to 5 heads out of 10 flips), given the size and the prob. Known as the cumulative probability distribution function or the cumulative density function.

**qbinom** - the quantile function: is the inverse of pbinom in that it gives you the x axis value below (and including the value) which the summation of probabilities is greater than or equal to a given probability p, plus given the size and prob.


So let's try these functions out to answer the two questions:

1. What is the probability of getting exactly 5 heads on 10 flips?
2. What is the probability of getting at most 2 heads on 10 flips?

Let's start with question 1, what is the probability of getting exactly 5 heads on 10 flips? 
So we want to predict the probability of getting 5 heads in 10 trials (coin flips) and the probability of success is 0.5 (it'll either be heads or tails so you have a 50/50 chance which we write as 0.5).

We write that as:

```{r binom_fake, results = FALSE}
dbinom(5, 10, 0.5)
```
The probability of getting 5 heads out of 10 coin flips is 0.25 (to 2 decimal places).

which would look like this:

``` {r binom_plot1, echo=FALSE, eval=TRUE}
barplot(dbinom(0:10, 10, 0.5), names.arg = 0:10, xlab = 'Number of Heads', ylab = 'Probability (p)' , col= c('purple', 'purple', 'purple', 'purple', 'purple','blue','purple', 'purple', 'purple', 'purple', 'purple' ))
```

The dbinom (density binom) function takes the format of dbinom(x, size, prob), where the arguments we give are:
* x the number of ‘heads’ we want to know the probability of. Either a single one, 3 or a series 0:10. In this case it's 5.
* size the number of trials (flips) we are doing; in this case, 10 flips.
* prob the probability of ‘heads’ on one trial. Here chance is 50-50 which as a probability we state as 0.5 or .5


OK, question  number 2....What is the probability of getting at most 2 heads on 10 flips? 
This time we use **pbinom** as we want to know the **cumulative probability** of getting a maximum of 2 heads from 10 coin flips so we have set a cut-off point of 2 but we still have a probability of getting a heads of 0.5.

```{r pbinom_fake, results = FALSE}
pbinom(2, 10, 0.5)
```

The probability of us getting a maximum of 2 heads on 10 coin flips is 0.05 (2 decimal places).

Note that there is another way we could have done this:
```{r pbinom_fake_range, results = FALSE}
probs <- pbinom(0:2, 10, 0.5)
probs
sum(probs)
```

Note that in this latter strategy, we are just adding up the heights of the three blue bars for the probabilities of getting 0,1 or 2 heads therefore calculating the cumulative probablity of this scenario.

``` {r binom_plot2, echo =FALSE, eval = TRUE}
barplot(dbinom(0:10, 10, 0.5), names.arg = 0:10, xlab = 'Number of Heads', ylab = 'Probability (p)' , col= c('blue', 'blue', 'blue', 'purple', 'purple', 'purple', 'purple', 'purple', 'purple', 'purple', 'purple' ))
```

Let's try one more scenario with a cut-off point to make sure we have this. What is the probability of getting 7 or more heads on 10 flips?
We can actually approach this in two different ways. We have done one way to answer the previous question which we can use again here but instead of using a maximum of 2 coin flips (0:2) we are setting the cut-off as 7 to 10 (7:10).

```{r pbinom_2, echo=FALSE, eval = TRUE}
sum(pbinom(7:10, 10, 0.5))
```

But we can also think about this in terms of the tails of the plots. Are we interested in the lower tail of the plot or the higher tail? Well, it depends on where our cut-off sits. What pbinom() gives us is the probability of getting 0 to 6 successes (the lower tail of the distribution, given by the blue bars). The total area under the curve for a theoretical distribution sums to 1. If we want the upper tail instead, we set lower.tail to FALSE, and this will give us the probability for the blue bars. In this example, our cut off is 7 up to 10 so we will set lower.tail = FALSE as we are interested in the upper tail of the distribution. 

```{r binom_2.5, echo=FALSE, eval = TRUE}
sum(pbinom(6, 10, .5, lower.tail = FALSE))
```

``` {r binom_plot3, echo =FALSE, eval = TRUE}
barplot(dbinom(0:10, 10, 0.5), names.arg = 0:10, xlab = 'Number of Heads', ylab = 'Probability (p)' , col= c('purple', 'purple', 'purple', 'purple', 'purple', 'purple', 'purple', 'blue', 'blue', 'blue', 'blue' ))
```

OK now let's consider a scenario in which you'd use the quantile function qbinom(). You suspect that the coin is biased against heads. Your null hypothesis is that the coin is not biased against heads (P(heads)=.5). You are going to run a single experiment to test your hypothesis, with 10 trials. What is the minimum number of successes that is acceptable if you want to keep your chances of getting heads for this type of experiment at .05?

```{r binom_3, echo=FALSE, eval = TRUE}
qbinom(.05, 10, .5)
```

So if you got less than two heads, you would reject the null that the coin was unbiased against heads.

Ten trials is probably far too few. What would your cutoff be if you ran 100 trials? 1000? 10000?

``` {r pbinom_3, echo=FALSE, eval = TRUE}
qbinom(.05, c(100, 1000, 10000), .5)
```

You have used probability in the previous two functions, dbinom and pbinom, and it represents the probability of success on a single trial (here it is the probability of 'heads' in one coin flip, .5). Now, probability represents the probability of success in one trial, whereas p represents the overall probability of success across all trials and gives you the number of heads that would give that probability. So you ask it for the minimum number of successes (e.g. heads) to maintain an overall probability of .05, in 10 flips, when the probability of a success on any one flip is .5. And it tells you the answer is 2. qbinom also uses the lower.tail argument and it works in a similar fashion to pbinom.

**Congratulations!** You are well on you way to understanding binomial probability so complete the assessment for this lab and submit one week from attending by 12pm. Next time we will be looking at normal distributions and probability.
